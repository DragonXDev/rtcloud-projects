{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6053a83-2259-475e-9e21-201e44217e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using storage path: /home/ri4541@pu.win.princeton.edu/rt_mindeye/rt_all_data\n"
     ]
    }
   ],
   "source": [
    "# set up main path where everything will be you should download the\n",
    "# hugging face directory described in readme and put it here on the same\n",
    "# server where the data analyzer is run so that the data analyzer code with \n",
    "# the GPU can access these files\n",
    "# You should replace the below path with your location\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    data_and_model_storage_path = config['data_and_model_storage_path']\n",
    "    bold_path = config['bold_path']\n",
    "    fsl_path = config['fsl_path']\n",
    "    assert os.path.exists(data_and_model_storage_path), \"The specified data and model storage path does not exist.\"\n",
    "    assert os.path.exists(bold_path), \"The specified BOLD path does not exist.\"\n",
    "    assert os.path.exists(fsl_path), \"The specified FSL path does not exist.\"\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"config.json file not found. Please create it with the required paths.\")\n",
    "\n",
    "print(f\"Using storage path: {data_and_model_storage_path}\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------------------\n",
    "Imports and set up for mindEye\n",
    "-----------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "# print(sys.path)\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# custom functions #\n",
    "import utils_mindeye\n",
    "from models import *\n",
    "import pandas as pd\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516e788-85cc-42ab-b05a-11bd7207f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir= f\"{data_and_model_storage_path}/cache\"\n",
    "model_name=\"sub-005_all_task-C_bs24_MST_rishab_MSTsplit_union_mask_finetune_0\"\n",
    "subj=1\n",
    "hidden_dim=1024\n",
    "blurry_recon = False\n",
    "n_blocks=4 \n",
    "seq_len = 1\n",
    "\n",
    "import pickle\n",
    "with open(f\"{data_and_model_storage_path}/clip_img_embedder\", \"rb\") as input_file:\n",
    "    clip_img_embedder = pickle.load(input_file)\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be1838-f387-4cdd-b7cb-217a74501359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "25,831,424 total\n",
      "25,831,424 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "479,191,704 total\n",
      "479,191,704 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "739,056,920 total\n",
      "739,056,904 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "739056904"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "num_voxels = 19174\n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=2, n_blocks=n_blocks, drop=.15, \n",
    "                clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "\n",
    "\n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "\n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b,t = torch.Tensor([0.]), torch.Tensor([[0.],[0.]]), torch.Tensor([0.])\n",
    "\n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                        clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils_mindeye.count_params(model.ridge)\n",
    "utils_mindeye.count_params(model.backbone)\n",
    "utils_mindeye.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "utils_mindeye.count_params(model.diffusion_prior)\n",
    "utils_mindeye.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a627d35-3cd5-4cd1-9bb3-c02c0c97f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "# Replace with pre_trained_fine_tuned_model.pth\n",
    "# tag='pretrained_fine-tuned_sliceTimed0.5.pth'\n",
    "# tag='pretrained_fine-tuned_sliceTimed.pth'\n",
    "tag='sub-005_all_task-C_bs24_MST_rishab_MSTsplit_union_mask_finetune_0.pth'\n",
    "outdir = os.path.abspath(f'{data_and_model_storage_path}/')\n",
    "\n",
    "# print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(outdir+f'/{tag}', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    del checkpoint\n",
    "except: # probably ckpt is saved using deepspeed format\n",
    "    import deepspeed\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "# print(\"ckpt loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bd11f3-6d4d-4ee4-a23b-443afeb5c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "# first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "# first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "with open(f\"{data_and_model_storage_path}/diffusion_engine\", \"rb\") as input_file:\n",
    "    diffusion_engine = pickle.load(input_file)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "    \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "    \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "f = h5py.File(f'{data_and_model_storage_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05736bc-c816-49ae-8718-b6c31b412781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn.glm.first_level import *\n",
    "from nilearn.image import get_data, index_img, concat_imgs, new_img_like\n",
    "\n",
    "# get the mask and the reference files\n",
    "ndscore_events = [pd.read_csv(f'{bold_path}/sub-005_ses-03_task-C_{run:02d}_events.tsv', sep = \"\\t\", header = 0) for run in range(1,2)]# create a new list of events_df's which will have the trial_type modified to be unique identifiers\n",
    "ndscore_tr_labels = [pd.read_csv(f\"{bold_path}/sub-005_ses-03_task-C_run-{run_num:02d}_tr_labels.csv\") for run_num in range(1,2)]\n",
    "# tr_length = 1.6  get TR length from the data\n",
    "mask_img = nib.load(f'{data_and_model_storage_path}/sub-01_nsdgeneral_to_day1ref.nii.gz')  # nsdgeneral mask in functional space\n",
    "day1_boldref= f\"{data_and_model_storage_path}/day1_bold_ref.nii.gz\" #day 1 reference image is the middle volume (vol0094) of day1run1\n",
    "day2_boldref= f\"{data_and_model_storage_path}/day2_bold_ref.nii.gz\" #day 2 reference image is the first volume (vol0000) of day2\n",
    "day2_to_day1_mat =  f\"{data_and_model_storage_path}/day2ref_to_day1ref\"\n",
    "def fast_apply_mask(target=None,mask=None):\n",
    "    return target[np.where(mask == 1)].T\n",
    "lss_glm = FirstLevelModel(t_r=tr_length,slice_time_ref=0,hrf_model='glover',\n",
    "                        drift_model='polynomial',high_pass=None,mask_img=mask_img,\n",
    "                        signal_scaling=False,smoothing_fwhm=None,noise_model='ar1',\n",
    "                        n_jobs=-1,verbose=-1,memory_level=1,minimize_memory=True)\n",
    "day1_boldref_nibd = nib.load(day1_boldref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895d9228-46e0-4ec0-9fe4-00f802f9708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reconstructions(betas_tt):\n",
    "    \"\"\"\n",
    "    takes in the beta map for a stimulus trial in torch tensor format (tt)\n",
    "\n",
    "    returns reconstructions and clipvoxels for retrievals\n",
    "    \"\"\"\n",
    "    # start_reconstruction_time = time.time()\n",
    "    model.to(device)\n",
    "    model.eval().requires_grad_(False)\n",
    "    clipvoxelsTR = None\n",
    "    reconsTR = None\n",
    "    num_samples_per_image = 1\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        voxel = betas_tt\n",
    "        voxel = voxel.to(device)\n",
    "        voxel_ridge = model.ridge(voxel[:,[0]],0) # 0th index of subj_list\n",
    "        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "        clip_voxels = clip_voxels0\n",
    "        backbone = backbone0\n",
    "        blurry_image_enc = blurry_image_enc0[0]\n",
    "        clipvoxelsTR = clip_voxels.cpu()\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)  \n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils_mindeye.unclip_recon(prior_out[[i]],\n",
    "                            diffusion_engine,\n",
    "                            vector_suffix,\n",
    "                            num_samples=num_samples_per_image)\n",
    "            if reconsTR is None:\n",
    "                reconsTR = samples.cpu()\n",
    "            else:\n",
    "                reconsTR = torch.vstack((reconsTR, samples.cpu()))\n",
    "            imsize = 224\n",
    "            reconsTR = transforms.Resize((imsize,imsize), antialias=True)(reconsTR).float().numpy().tolist()\n",
    "        return reconsTR, clipvoxelsTR\n",
    "    \n",
    "def batchwise_cosine_similarity(Z,B):\n",
    "    Z = Z.flatten(1)\n",
    "    B = B.flatten(1).T\n",
    "    Z_norm = torch.linalg.norm(Z, dim=1, keepdim=True)  # Size (n, 1).\n",
    "    B_norm = torch.linalg.norm(B, dim=0, keepdim=True)  # Size (1, b).\n",
    "    cosine_similarity = ((Z @ B) / (Z_norm @ B_norm)).T\n",
    "    return cosine_similarity\n",
    "\n",
    "def get_top_retrievals(clipvoxel, all_images, stimulus_trial_counter, total_retrievals = 1):\n",
    "    '''\n",
    "    clipvoxel: output from do_recons that contains that information needed for retrievals\n",
    "    all_images: all ground truth actually seen images by the participant in day 2 run 1\n",
    "\n",
    "    outputs the top retrievals\n",
    "    '''\n",
    "    values_dict = {}\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        emb = clip_img_embedder(torch.reshape(all_images,(all_images.shape[0], 3, 224, 224)).to(device)).float() # CLIP-Image\n",
    "        emb = emb.cpu()\n",
    "        emb_ = clipvoxel # CLIP-Brain\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = np.reshape(emb_, (1, 425984))\n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "        emb_ = emb_.float()\n",
    "        fwd_sim = batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "        print(\"Given Brain embedding, find correct Image embedding\")\n",
    "    fwd_sim = np.array(fwd_sim.cpu())\n",
    "    imsize = 224\n",
    "    values_dict[\"ground_truth\"] = transforms.Resize((imsize,imsize), antialias=True)(all_images[stimulus_trial_counter]).float().numpy().tolist()\n",
    "    # values_dict[\"ground_truth\"] = all_images[stimulus_trial_counter].numpy().tolist()\n",
    "    for attempt in range(total_retrievals):\n",
    "        which = np.flip(np.argsort(fwd_sim, axis = 0))[attempt]\n",
    "       # values_dict[f\"attempt{(attempt+1)}\"] = all_images[which.copy()].numpy().tolist()\n",
    "        values_dict[f\"attempt{(attempt+1)}\"] = transforms.Resize((imsize,imsize), antialias=True)(all_images[which.copy()]).float().numpy().tolist()\n",
    "    return values_dict\n",
    "\n",
    "\n",
    "def convert_image_array_to_PIL(image_array):\n",
    "    if image_array.ndim == 4:\n",
    "        image_array = image_array[0]\n",
    "\n",
    "    # get the dimension to h, w, 3|1\n",
    "    if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "    \n",
    "    # clip the image array to 0-1\n",
    "    image_array = np.clip(image_array, 0, 1)\n",
    "    # convert the image array to uint8\n",
    "    image_array = (image_array * 255).astype('uint8')\n",
    "    # convert the image array to PIL\n",
    "    return Image.fromarray(image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a4d40-643d-493b-874b-2030490b9bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 started\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m bold \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(bold_path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m TR \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bold\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtr_labels_hrf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTR\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen_label_before:\n\u001b[1;32m     22\u001b[0m         seen_label_before\u001b[38;5;241m.\u001b[39mappend(tr_labels_hrf[TR])\n\u001b[1;32m     23\u001b[0m         image_COCO_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(tr_labels_hrf[TR]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# set the output type to NIFTI_GZ\n",
    "os.environ['FSLOUTPUTTYPE'] = 'NIFTI_GZ'\n",
    "\n",
    "run_num = 1\n",
    "print(f\"{run_num} started\")\n",
    "mc_params = []\n",
    "imgs = []\n",
    "events_df = ndscore_events[run_num - 1]\n",
    "tr_labels_hrf = ndscore_tr_labels[run_num - 1][\"tr_label_hrf\"].tolist()\n",
    "beta_maps_list = []\n",
    "all_trial_names_list = []\n",
    "# get the all images tensor\n",
    "all_images = None\n",
    "seen_label_before = [\"blank\"]\n",
    "# get the list of all images in torch tensor format for this run (should be 62 or 63 images)\n",
    "all_COCO_ids = []\n",
    "\n",
    "bold = nib.load(f\"{bold_path}/sub-005_ses-03_task-C_run-01_bold.nii.gz\")\n",
    "\n",
    "for TR in range(bold.shape[3]):\n",
    "    if tr_labels_hrf[TR] not in seen_label_before:\n",
    "        seen_label_before.append(tr_labels_hrf[TR])\n",
    "        image_COCO_id = int(float(tr_labels_hrf[TR].split(\"_\")[1])) - 1\n",
    "        new_image_pt = torch.from_numpy(np.reshape(images[image_COCO_id],(1,3,224,224)))\n",
    "        all_images = new_image_pt if all_images == None else torch.vstack((all_images, new_image_pt))\n",
    "        all_COCO_ids.append(image_COCO_id)\n",
    "# print(all_COCO_ids)\n",
    "\n",
    "\n",
    "# define save_path\n",
    "save_path = f\"{data_and_model_storage_path}/sub-01_ses-nsd02_task-nsdcore_run-{run_num:02d}_bold_recons\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "save_individual_images = True\n",
    "if save_individual_images:\n",
    "    os.makedirs(os.path.join(save_path, \"individual_images\"), exist_ok=True)\n",
    "\n",
    "all_recons_save = []\n",
    "all_clipvoxels_save = []\n",
    "all_ground_truth_save = []\n",
    "all_retrieved_save = []\n",
    "\n",
    "\n",
    "stimulus_trial_counter = 0\n",
    "plot_images = True\n",
    "\n",
    "try:\n",
    "    for TR in range(bold.shape[3]):\n",
    "        print(f\"TR {TR}\")\n",
    "        # stream in the nifti\n",
    "        image_data = bold.slicer[:,:,:,None,TR]  # None prevents the final dimension from being dropped because it's a singleton; nibabel methods expect a 4D array\n",
    "        current_label = tr_labels_hrf[TR]\n",
    "        if TR == 0:\n",
    "            day2_run1_bold_ref = image_data\n",
    "            # make the day 2 bold ref\n",
    "            nib.save(image_data, day2_boldref)\n",
    "            # save the transformation from the day 2 bold ref to the day 1 \n",
    "            os.system(f\"{fsl_path}/flirt -in {day2_boldref} \\\n",
    "            -ref {day1_boldref} \\\n",
    "            -omat {day2_to_day1_mat} \\\n",
    "            -dof 6\")\n",
    "        # load nifti file\n",
    "        tmp = f'{data_and_model_storage_path}/day2_subj1/tmp_run{run_num}.nii.gz'\n",
    "        nib.save(index_img(image_data,0),tmp)\n",
    "        start = time.time()\n",
    "        # on first tr the motion correction will have no issue so that mc_params is properly populated\n",
    "        mc = f'{data_and_model_storage_path}/day2_subj1/tmp_mc_run{run_num}'\n",
    "        os.system(f\"{fsl_path}/mcflirt -in {tmp} -reffile {day2_boldref} -out {mc} -plots -mats\")\n",
    "        mc_params.append(np.loadtxt(f'{mc}.par'))\n",
    "\n",
    "        slice_timed = f'{data_and_model_storage_path}/day2_subj1/tmp_sT_run{run_num}'\n",
    "        slice_tcustom_path = f'{data_and_model_storage_path}/slice_timing_day2_run1.txt'\n",
    "        os.system(f\"{fsl_path}/slicetimer -i {tmp} -o {slice_timed} --tcustom={slice_tcustom_path}\")\n",
    "\n",
    "        mc_day1_aligned = f'{data_and_model_storage_path}/day2_subj1/tmp_mc_day1_aligned_run{run_num}'\n",
    "        current_tr_to_day1 = f\"{data_and_model_storage_path}/day2_subj1/current_tr_to_day1_run{run_num}\"\n",
    "        os.system(f\"{fsl_path}/convert_xfm -concat {day2_to_day1_mat} -omat {current_tr_to_day1} {mc}.mat/MAT_0000\")    \n",
    "        # apply concatenated matrix to the current TR\n",
    "        os.system(f\"{fsl_path}/flirt -in {slice_timed} \\\n",
    "        -ref {day1_boldref} \\\n",
    "        -out {mc_day1_aligned} \\\n",
    "        -init {current_tr_to_day1} \\\n",
    "        -applyxfm\")\n",
    "        # now delete the mc from current tr to bold reference mat\n",
    "        os.system(f\"rm -r {mc}.mat\") \n",
    "        imgs.append(get_data(mc_day1_aligned + \".nii.gz\")) # only add to imgs list\n",
    "        if tr_labels_hrf[TR] != tr_labels_hrf[TR + 1] and tr_labels_hrf[TR] != \"blank\":\n",
    "            print('last TR of real image')\n",
    "            cropped_events = events_df[events_df.trial_number <= int(float(tr_labels_hrf[TR].split(\"_\")[3]))].astype(str)\n",
    "            for i_trial, trial in cropped_events.iterrows():\n",
    "                cropped_events.loc[i_trial, \"trial_type\"] = \"reference\" if i_trial < (len(cropped_events) - 1) else \"probe\"\n",
    "                \n",
    "            cropped_events = cropped_events.drop(columns=['total_novel_presses', 'change_mind', 'is_correct', 'time', \n",
    "                                                'response_time', 'response', '73k_id', 'trial_number', \n",
    "                                                '10k_id', 'memory_first', 'is_old_session', 'is_correct_session', \n",
    "                                                'missing_data', 'total_old_presses', 'memory_recent'])  # nilearn glm fit requires onset and duration; trial_type is optional (https://nilearn.github.io/stable/modules/generated/nilearn.glm.first_level.make_first_level_design_matrix.html#nilearn.glm.first_level.make_first_level_design_matrix)\n",
    "\n",
    "            # get the image id from this stimulus trial that we are fitting a model on\n",
    "            image_COCO_id = int(float(tr_labels_hrf[TR].split(\"_\")[1])) - 1\n",
    "            # collect all of the images at each TR into a 4D time series\n",
    "            img = np.rollaxis(np.array(imgs),0,4)\n",
    "            img = new_img_like(day1_boldref_nibd,img,copy_header=True)\n",
    "            # run the model with mc_params confounds to motion correct\n",
    "            lss_glm.fit(run_imgs=img,events=cropped_events, confounds = pd.DataFrame(np.array(mc_params)))\n",
    "            # get the beta map and mask it\n",
    "            beta_map = lss_glm.compute_contrast(\"probe\", output_type=\"effect_size\")\n",
    "            beta_map_np = beta_map.get_fdata()\n",
    "            beta_map_np = fast_apply_mask(target=beta_map_np,mask=mask_img.get_fdata())\n",
    "            beta_map_np = np.reshape(beta_map_np, (1,1,25225))\n",
    "            betas_tt = torch.Tensor(beta_map_np).to(\"cpu\")\n",
    "            new_image_pt = torch.from_numpy(images[image_COCO_id])\n",
    "            reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "            values_dict = get_top_retrievals(clipvoxelsTR, all_images=all_images, stimulus_trial_counter=stimulus_trial_counter, total_retrievals=5)\n",
    "            image_array = np.array(reconsTR)[0]\n",
    "            # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "            if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "            # Display the image\n",
    "            if plot_images:\n",
    "                plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                plt.axis('off')  # Hide axes\n",
    "                plt.show()\n",
    "\n",
    "            # subjInterface.setResultDict allows us to send to the analysis listener immediately\n",
    "            # subjInterface.setResultDict(name=f'run{run_num}_TR{TR}',\n",
    "            #                             values=values_dict)\n",
    "            stimulus_trial_counter += 1\n",
    "            \n",
    "            # save reconstructed image, retrieved images, clip_voxels, and ground truth image\n",
    "            if save_individual_images:\n",
    "                # save the reconstructed image\n",
    "                convert_image_array_to_PIL(image_array).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_reconstructed.png\"))\n",
    "                # save the retrieved images\n",
    "                for key, value in values_dict.items():\n",
    "                    if (not ('ground_truth' in key)):\n",
    "                        convert_image_array_to_PIL(np.array(value)).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_retrieved_{key}.png\"))\n",
    "                # save the clip_voxels\n",
    "                np.save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_clip_voxels.npy\"), clipvoxelsTR)\n",
    "                # save the ground truth image\n",
    "                convert_image_array_to_PIL(np.array(values_dict[\"ground_truth\"])).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_ground_truth.png\"))\n",
    "\n",
    "            all_recons_save.append(image_array)\n",
    "            all_clipvoxels_save.append(clipvoxelsTR)\n",
    "            all_ground_truth_save.append(np.array(values_dict[\"ground_truth\"]))\n",
    "            all_retrieved_save.append([np.array(value) for key, value in values_dict.items() if (not ('ground_truth' in key))])\n",
    "\n",
    "        \n",
    "        else:\n",
    "            if tr_labels_hrf[TR] != \"blank\":\n",
    "                print('non-last TR of real image')\n",
    "                values_dict = {}\n",
    "                image_COCO_id = int(float(tr_labels_hrf[TR].split(\"_\")[1])) - 1\n",
    "                imsize = 224\n",
    "                values_dict[\"ground_truth\"] = transforms.Resize((imsize,imsize), antialias=True)(all_images[stimulus_trial_counter]).float().numpy().tolist()\n",
    "                image_array = np.array(values_dict[\"ground_truth\"])\n",
    "\n",
    "                # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "                if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                    image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "                # Display the image\n",
    "                if plot_images:\n",
    "                    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.show()\n",
    "                # subjInterface.setResultDict(name=f'run{run_num}_TR{TR}',\n",
    "                #                             values=values_dict)\n",
    "            else:\n",
    "                print('blank')\n",
    "                pass\n",
    "                # when we are not at the end of a stimulus trial, send an empty dictionary to the analysis listener with \"pass\"\n",
    "                # subjInterface.setResultDict(name=f'run{run_num}_TR{TR}',\n",
    "                #                 values={'pass': \"pass\"})\n",
    "\n",
    "    print(f\"==END OF RUN {run_num}!==\\n\")\n",
    "    # save the tensors\n",
    "    all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "    all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "    all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "    all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "    torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "    torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "    torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "    torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "    print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "    print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "    print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "    print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "    print(\"All tensors saved successfully on \", save_path)\n",
    "\n",
    "\n",
    "except:\n",
    "    # save the tensors\n",
    "    all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "    all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "    all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "    all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "    torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "    torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "    torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "    torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "    print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "    print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "    print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "    print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "    print(\"All tensors saved successfully on \", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6675825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_486579/2069072826.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_recons_save_tensor = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16).to(device)\n",
      "/tmp/ipykernel_486579/2069072826.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_clipvoxels_save_tensor = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16).to(device)\n",
      "/tmp/ipykernel_486579/2069072826.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_ground_truth_save_tensor = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16).to(device)\n",
      "INFO:root:Loaded ViT-bigG-14 model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated images averaged:  []\n",
      "Loading clip_img_embedder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading pretrained ViT-bigG-14 weights (laion2b_s39b_b160k).\n",
      "/home/ubuntu/rt_mindEye2/lib/python3.11/site-packages/open_clip/factory.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "/home/ubuntu/rtcloud-projects/mindeye/utils_mindeye.py:359: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total pool of images and clip voxels to do retrieval on is:  6\n",
      "Creating embeddings for images\n",
      "Calculating retrieval metrics\n",
      "overall fwd percent_correct: 0.3333\n",
      "overall bwd percent_correct: 0.3333\n",
      "torch.Size([6, 541875])\n",
      "torch.Size([6, 541875])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 100.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Correlation: 0.09440470286892115\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 44.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.23450645218319544\n",
      "Loading AlexNet\n",
      "\n",
      "---early, AlexNet(2)---\n",
      "2-way Percent Correct (early AlexNet): 0.5000\n",
      "\n",
      "---mid, AlexNet(5)---\n",
      "2-way Percent Correct (mid AlexNet): 0.5000\n",
      "Loading Inception V3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rt_mindEye2/lib/python3.11/site-packages/torchvision/models/feature_extraction.py:174: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
      "  warnings.warn(msg + suggestion_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-way Percent Correct (Inception V3): 0.3667\n",
      "Loading CLIP\n",
      "2-way Percent Correct (CLIP): 0.6333\n",
      "Loading EfficientNet B1\n",
      "Distance EfficientNet B1: 0.9910986612131102\n",
      "Loading SwAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_swav_main\n",
      "/home/ubuntu/rt_mindEye2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/rt_mindEye2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance SwAV: 0.6458645651918116\n",
      "                 Value\n",
      "Metric                \n",
      "alexnet2      0.500000\n",
      "alexnet5      0.500000\n",
      "inception     0.366667\n",
      "clip_         0.633333\n",
      "efficientnet  0.991099\n",
      "swav          0.645865\n",
      "pixcorr       0.094405\n",
      "ssim          0.234506\n",
      "all_fwd_acc   0.333333\n",
      "all_bwd_acc   0.333333\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation metrics\n",
    "from utils_mindeye import calculate_retrieval_metrics, calculate_alexnet, calculate_clip, calculate_swav, calculate_efficientnet_b1, calculate_inception_v3, calculate_pixcorr, calculate_ssim, deduplicate_tensors\n",
    "\n",
    "# run_num = 1\n",
    "# save_path = f\"{data_and_model_storage_path}/sub-01_ses-nsd02_task-nsdcore_run-{run_num:02d}_bold_recons\"\n",
    "\n",
    "try:\n",
    "    all_recons_save_tensor = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16).to(device)\n",
    "    all_clipvoxels_save_tensor = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16).to(device)\n",
    "    all_ground_truth_save_tensor = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16).to(device)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Tensors not found. Please check the save path.\")\n",
    "\n",
    "\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    # For retrieval metrics we need to average the clip_voxels of duplicate images    \n",
    "    unique_clip_voxels, unique_ground_truth, duplicated = deduplicate_tensors(all_clipvoxels_save_tensor, all_ground_truth_save_tensor)\n",
    "    print(\"Duplicated images averaged: \", duplicated)\n",
    "    all_fwd_acc, all_bwd_acc = calculate_retrieval_metrics(unique_clip_voxels, unique_ground_truth)\n",
    "    pixcorr = calculate_pixcorr(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    ssim_ = calculate_ssim(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    alexnet2, alexnet5 = calculate_alexnet(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    inception = calculate_inception_v3(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    clip_ = calculate_clip(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    efficientnet = calculate_efficientnet_b1(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    swav = calculate_swav(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "\n",
    "# save the results to a csv file\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"alexnet2\",\n",
    "        \"alexnet5\",\n",
    "        \"inception\",\n",
    "        \"clip_\",\n",
    "        \"efficientnet\",\n",
    "        \"swav\",\n",
    "        \"pixcorr\",\n",
    "        \"ssim\",\n",
    "        \"all_fwd_acc\",\n",
    "        \"all_bwd_acc\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        alexnet2,\n",
    "        alexnet5,\n",
    "        inception,\n",
    "        clip_,\n",
    "        efficientnet,\n",
    "        swav,\n",
    "        pixcorr,\n",
    "        ssim_,\n",
    "        all_fwd_acc,\n",
    "        all_bwd_acc\n",
    "    ]\n",
    "})\n",
    "df_metrics.set_index(\"Metric\", inplace=True)\n",
    "print(df_metrics)\n",
    "df_metrics.to_csv(os.path.join(save_path, \"metrics.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt_mindEye2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
