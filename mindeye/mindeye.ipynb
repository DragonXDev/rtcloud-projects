{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6053a83-2259-475e-9e21-201e44217e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using storage path: /home/ri4541@pu.win.princeton.edu/rt_mindeye/rt_all_data\n",
      "line 6:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n",
      "line 6:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n",
      "line 14:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n",
      "line 14:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n"
     ]
    }
   ],
   "source": [
    "# set up main path where everything will be you should download the\n",
    "# hugging face directory described in readme and put it here on the same\n",
    "# server where the data analyzer is run so that the data analyzer code with \n",
    "# the GPU can access these files\n",
    "# You should replace the below path with your location\n",
    "import json\n",
    "import os\n",
    "try:\n",
    "    with open('conf/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    storage_path = config['storage_path']\n",
    "    data_path = config['data_path']\n",
    "    derivatives_path = config['derivatives_path']\n",
    "    fsl_path = config['fsl_path']\n",
    "    assert os.path.exists(storage_path), \"The specified data and model storage path does not exist.\"\n",
    "    assert os.path.exists(data_path), \"The specified BOLD path does not exist.\"\n",
    "    assert os.path.exists(derivatives_path), \"The specified derivatives path does not exist.\"\n",
    "    assert os.path.exists(fsl_path), \"The specified FSL path does not exist.\"\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"config.json file not found. Please create it with the required paths.\")\n",
    "\n",
    "print(f\"Using storage path: {storage_path}\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------------------\n",
    "Imports and set up for mindEye\n",
    "-----------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "# print(sys.path)\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# custom functions #\n",
    "import utils_mindeye\n",
    "from models import *\n",
    "import pandas as pd\n",
    "import ants\n",
    "import nilearn\n",
    "import pdb\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4516e788-85cc-42ab-b05a-11bd7207f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir= f\"{storage_path}/cache\"\n",
    "model_name=\"sub-005_all_task-C_bs24_MST_rishab_MSTsplit_union_mask_finetune_0\"\n",
    "subj=1\n",
    "hidden_dim=1024\n",
    "blurry_recon = False\n",
    "n_blocks=4 \n",
    "seq_len = 1\n",
    "\n",
    "import pickle\n",
    "with open(f\"{storage_path}/clip_img_embedder\", \"rb\") as input_file:\n",
    "    clip_img_embedder = pickle.load(input_file)\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12be1838-f387-4cdd-b7cb-217a74501359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "8,835,072 total\n",
      "8,835,072 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "462,195,352 total\n",
      "462,195,352 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "722,060,568 total\n",
      "722,060,552 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "722060552"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "num_voxels = 8627\n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=2, n_blocks=n_blocks, drop=.15, \n",
    "                clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "\n",
    "\n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "\n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b,t = torch.Tensor([0.]), torch.Tensor([[0.],[0.]]), torch.Tensor([0.])\n",
    "\n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                        clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils_mindeye.count_params(model.ridge)\n",
    "utils_mindeye.count_params(model.backbone)\n",
    "utils_mindeye.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "utils_mindeye.count_params(model.diffusion_prior)\n",
    "utils_mindeye.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a627d35-3cd5-4cd1-9bb3-c02c0c97f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "# Replace with pre_trained_fine_tuned_model.pth\n",
    "# tag='pretrained_fine-tuned_sliceTimed0.5.pth'\n",
    "# tag='pretrained_fine-tuned_sliceTimed.pth'\n",
    "tag='sub-005_all_task-C_bs24_MST_rishab_MSTsplit_union_mask_finetune_0.pth'\n",
    "outdir = os.path.abspath(f'3t/data/model')\n",
    "\n",
    "# print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "# try:\n",
    "checkpoint = torch.load(outdir+f'/{tag}', map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "del checkpoint\n",
    "# except: # probably ckpt is saved using deepspeed format\n",
    "#     import deepspeed\n",
    "#     state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "#     model.load_state_dict(state_dict, strict=False)\n",
    "#     del state_dict\n",
    "# print(\"ckpt loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bd11f3-6d4d-4ee4-a23b-443afeb5c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "# first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "# first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "with open(f\"{storage_path}/diffusion_engine\", \"rb\") as input_file:\n",
    "    diffusion_engine = pickle.load(input_file)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "    \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "    \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "# f = h5py.File(f'{storage_path}/coco_images_224_float16.hdf5', 'r')\n",
    "# images = f['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58cb6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub = \"sub-005\"\n",
    "session = \"ses-03\"\n",
    "task = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "func_task_name = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "n_runs = 11\n",
    "\n",
    "ses_list = [session]\n",
    "design_ses_list = [session]\n",
    "    \n",
    "task_name = f\"_task-{task}\" if task != 'study' else ''\n",
    "designdir = f\"{data_path}/events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48586a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (780, 126)\n",
      "Using design file: /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/events/csv/sub-005_ses-03.csv\n",
      "Total number of images: 770\n",
      "Number of unique images: 532\n",
      "n_runs 11\n",
      "['all_stimuli/unchosen_nsd_1000_images/unchosen_7211_cocoid_59250.png'\n",
      " 'all_stimuli/special515/special_67295.jpg'\n",
      " 'all_stimuli/unchosen_nsd_1000_images/unchosen_5729_cocoid_53029.png'\n",
      " 'all_stimuli/special515/special_70232.jpg']\n",
      "[174.7109683 178.7049172 182.7072832 186.7297016]\n",
      "[0. 0. 0. 0.]\n",
      "(693,)\n"
     ]
    }
   ],
   "source": [
    "data, starts, images, is_new_run, image_names, unique_images, len_unique_images = utils_mindeye.load_design_files(\n",
    "    sub=sub,\n",
    "    session=session,\n",
    "    func_task_name=task,\n",
    "    designdir=designdir,\n",
    "    design_ses_list=design_ses_list\n",
    ")\n",
    "\n",
    "if sub == 'sub-001':\n",
    "    if session == 'ses-01':\n",
    "        assert image_names[0] == 'images/image_686_seed_1.png'\n",
    "    elif session in ('ses-02', 'all'):\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_40840.jpg'\n",
    "    elif session == 'ses-03':\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_69839.jpg'\n",
    "    elif session == 'ses-04':\n",
    "        assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "elif sub == 'sub-003':\n",
    "    assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "\n",
    "unique_images = np.unique(image_names.astype(str))\n",
    "unique_images = unique_images[(unique_images!=\"nan\")]\n",
    "len_unique_images = len(unique_images)\n",
    "print(\"n_runs\",n_runs)\n",
    "\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(unique_images) == 851\n",
    "\n",
    "print(image_names[:4])\n",
    "print(starts[:4])\n",
    "print(is_new_run[:4])\n",
    "\n",
    "image_idx = np.array([])  # contains the unique index of each presented image\n",
    "vox_image_names = np.array([])  # contains the names of the images corresponding to image_idx\n",
    "all_MST_images = dict()\n",
    "for i, im in enumerate(image_names):\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    vox_image_names = np.append(vox_image_names, im)\n",
    "            \n",
    "    image_idx_ = np.where(im==unique_images)[0].item()\n",
    "    image_idx = np.append(image_idx, image_idx_)\n",
    "    \n",
    "    all_MST_images[i] = im\n",
    "    i+=1\n",
    "    \n",
    "image_idx = torch.Tensor(image_idx).long()\n",
    "# for im in new_image_names[MST_images]:\n",
    "#     assert 'MST_pairs' in im\n",
    "# assert len(all_MST_images) == 300\n",
    "\n",
    "unique_MST_images = np.unique(list(all_MST_images.values())) \n",
    "\n",
    "MST_ID = np.array([], dtype=int)\n",
    "\n",
    "vox_idx = np.array([], dtype=int)\n",
    "j=0  # this is a counter keeping track of the remove_random_n used later to index vox based on the removed images; unused otherwise\n",
    "for i, im in enumerate(image_names):  # need unique_MST_images to be defined, so repeating the same loop structure\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    j+=1\n",
    "    curr = np.where(im == unique_MST_images)\n",
    "    # print(curr)\n",
    "    if curr[0].size == 0:\n",
    "        MST_ID = np.append(MST_ID, np.array(len(unique_MST_images)))  # add a value that should be out of range based on the for loop, will index it out later\n",
    "    else:\n",
    "        MST_ID = np.append(MST_ID, curr)\n",
    "        \n",
    "assert len(MST_ID) == len(image_idx)\n",
    "# assert len(np.argwhere(pd.isna(data['current_image']))) + len(np.argwhere(data['current_image'] == 'blank.jpg')) + len(image_idx) == len(data)\n",
    "# MST_ID = torch.tensor(MST_ID[MST_ID != len(unique_MST_images)], dtype=torch.uint8)  # torch.tensor (lowercase) allows dtype kwarg, Tensor (uppercase) is an alias for torch.FloatTensor\n",
    "print(MST_ID.shape)\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(all_MST_images) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d16a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 0/693 [00:00<?, ?it/s]/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 693/693 [00:06<00:00, 104.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images torch.Size([693, 3, 224, 224])\n",
      "len MST_images 693\n",
      "MST_images==True 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "MST_images = []\n",
    "images = None\n",
    "for im_name in tqdm(image_idx):\n",
    "    image_file = f\"{unique_images[im_name]}\"\n",
    "    im = imageio.imread(f\"{data_path}/{image_file}\")\n",
    "    im = torch.Tensor(im / 255).permute(2,0,1)\n",
    "    im = resize_transform(im.unsqueeze(0))\n",
    "    if images is None:\n",
    "        images = im\n",
    "    else:\n",
    "        images = torch.vstack((images, im))\n",
    "    if (\"MST_pairs\" in image_file): # (\"_seed_\" not in unique_images[im_name]) and (unique_images[im_name] != \"blank.jpg\") \n",
    "        MST_images.append(True)\n",
    "    else:\n",
    "        MST_images.append(False)\n",
    "\n",
    "print(\"images\", images.shape)\n",
    "MST_images = np.array(MST_images)\n",
    "print(\"len MST_images\", len(MST_images))\n",
    "assert len(MST_images[MST_images==True]) == 124\n",
    "print(\"MST_images==True\", len(MST_images[MST_images==True]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f692b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pairs(sub, session, func_task_name, designdir):\n",
    "    \"\"\"Loads design files and processes image pairs for a given session.\"\"\"\n",
    "    _, _, _, _, image_names, unique_images, _ = utils_mindeye.load_design_files(\n",
    "        sub=sub,\n",
    "        session=session,\n",
    "        func_task_name=func_task_name,\n",
    "        designdir=designdir,\n",
    "        design_ses_list=[session]  # Ensure it's a list\n",
    "    )\n",
    "    return utils_mindeye.process_images(image_names, unique_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbffed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (780, 126)\n",
      "Using design file: /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/events/csv/sub-005_ses-03.csv\n",
      "Total number of images: 770\n",
      "Number of unique images: 532\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_dicts = []\n",
    "for s_idx, s in enumerate(ses_list):\n",
    "    im, vo, _ = get_image_pairs(sub, s, func_task_name, designdir)\n",
    "    assert len(im) == len(vo)\n",
    "    all_dicts.append({k:v for k,v in enumerate(vo)})\n",
    "\n",
    "assert session == 'ses-03'\n",
    "image_to_indices = defaultdict(lambda: [[] for _ in range(len(ses_list))])\n",
    "for ses_idx, idx_to_name in enumerate(all_dicts):\n",
    "    for idx, name in idx_to_name.items():\n",
    "        image_to_indices[name][ses_idx].append(idx)\n",
    "        \n",
    "image_to_indices = dict(image_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b2474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 124\n",
      "MST_idx (62,)\n"
     ]
    }
   ],
   "source": [
    "utils_mindeye.seed_everything(0)\n",
    "MST_idx = np.array([v for k,v in image_to_indices.items() if 'MST_pairs' in k])\n",
    "# train_image_indices = np.array([]).astype(np.int8)\n",
    "# test_image_indices = np.concatenate([np.where(MST_images == True)[0], np.where(MST_images == False)[0]])\n",
    "train_image_indices = np.where(MST_images == False)[0]\n",
    "test_image_indices = np.where(MST_images == True)[0]\n",
    "print(len(train_image_indices), len(test_image_indices))\n",
    "\n",
    "# for now, use the first subset of repeats only\n",
    "MST_idx = MST_idx[:,0,0]\n",
    "print(\"MST_idx\", MST_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05736bc-c816-49ae-8718-b6c31b412781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.image import get_data, index_img, concat_imgs, new_img_like\n",
    "\n",
    "# bold = nib.load(f\"{data_path}/sub-005_ses-03_task-C_run-01_bold.nii.gz\")  # shouldn't load all the data \n",
    "\n",
    "# get the mask and the reference files\n",
    "ndscore_events = [pd.read_csv(f'{data_path}/events/sub-005_ses-03_task-C_run-{run+1:02d}_events.tsv', sep = \"\\t\", header = 0) for run in range(n_runs)]  # create a new list of events_df's which will have the trial_type modified to be unique identifiers\n",
    "ndscore_tr_labels = [pd.read_csv(f\"{data_path}/events/sub-005_ses-03_task-C_run-{run+1:02d}_tr_labels.csv\") for run in range(n_runs)]\n",
    "tr_length = 1.5\n",
    "mask_img = nib.load(f'{data_path}/sub-005_final_mask.nii.gz')  # nsdgeneral mask in functional space\n",
    "ses1_boldref= f\"{data_path}/sub-005_ses-01_task-C_run-01_space-T1w_boldref.nii.gz\"  # preprocessed boldref from ses-01\n",
    "# ses3_vol0= f\"{data_path}/../derivatives/vols/sub-005_ses-03_task-C_run-01_bold_0000.nii.gz\" # first volume (vol0000) of real-time session\n",
    "# day2_to_day1_mat =  f\"{storage_path}/day2ref_to_day1ref\"\n",
    "def fast_apply_mask(target=None,mask=None):\n",
    "    return target[np.where(mask == 1)].T\n",
    "ses1_boldref_nib = nib.load(ses1_boldref)\n",
    "union_mask = np.load(f\"{data_path}/union_mask_from_ses-01-02.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21c9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply union mask to the nsdgeneral ROI and convert to nifti\n",
    "assert mask_img.get_fdata().sum() == union_mask.shape\n",
    "union_mask_img = new_img_like(mask_img, union_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4768e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply union_mask to mask_img and return nifti object\n",
    "\n",
    "# Get the data as a boolean array\n",
    "mask_data = mask_img.get_fdata().astype(bool)\n",
    "\n",
    "# Flatten only the True voxels in the mask\n",
    "true_voxel_indices = np.where(mask_data.ravel())[0]\n",
    "\n",
    "# Apply the union_mask (boolean mask of size 19174)\n",
    "selected_voxel_indices = true_voxel_indices[union_mask]\n",
    "\n",
    "# Create a new flattened mask with all False\n",
    "new_mask_flat = np.zeros(mask_data.size, dtype=bool)\n",
    "\n",
    "# Set selected voxels to True\n",
    "new_mask_flat[selected_voxel_indices] = True\n",
    "\n",
    "# Reshape back to original 3D shape\n",
    "new_mask_data = new_mask_flat.reshape(mask_data.shape)\n",
    "\n",
    "# Create new NIfTI image\n",
    "union_mask_img = nib.Nifti1Image(new_mask_data.astype(np.uint8), affine=mask_img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "057b3dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union_mask_img.shape (76, 90, 74)\n",
      "union mask num voxels 8627\n"
     ]
    }
   ],
   "source": [
    "print(\"union_mask_img.shape\", union_mask_img.shape)\n",
    "print(\"union mask num voxels\", int(union_mask_img.get_fdata().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895d9228-46e0-4ec0-9fe4-00f802f9708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reconstructions(betas_tt):\n",
    "    \"\"\"\n",
    "    takes in the beta map for a stimulus trial in torch tensor format (tt)\n",
    "\n",
    "    returns reconstructions and clipvoxels for retrievals\n",
    "    \"\"\"\n",
    "    # start_reconstruction_time = time.time()\n",
    "    model.to(device)\n",
    "    model.eval().requires_grad_(False)\n",
    "    clipvoxelsTR = None\n",
    "    reconsTR = None\n",
    "    num_samples_per_image = 1\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        voxel = betas_tt\n",
    "        voxel = voxel.to(device)\n",
    "        voxel_ridge = model.ridge(voxel[:,[0]],0) # 0th index of subj_list\n",
    "        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "        clip_voxels = clip_voxels0\n",
    "        backbone = backbone0\n",
    "        blurry_image_enc = blurry_image_enc0[0]\n",
    "        clipvoxelsTR = clip_voxels.cpu()\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)  \n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils_mindeye.unclip_recon(prior_out[[i]],\n",
    "                            diffusion_engine,\n",
    "                            vector_suffix,\n",
    "                            num_samples=num_samples_per_image)\n",
    "            if reconsTR is None:\n",
    "                reconsTR = samples.cpu()\n",
    "            else:\n",
    "                reconsTR = torch.vstack((reconsTR, samples.cpu()))\n",
    "            imsize = 224\n",
    "            reconsTR = transforms.Resize((imsize,imsize), antialias=True)(reconsTR).float().numpy().tolist()\n",
    "        return reconsTR, clipvoxelsTR\n",
    "    \n",
    "def batchwise_cosine_similarity(Z,B):\n",
    "    Z = Z.flatten(1)\n",
    "    B = B.flatten(1).T\n",
    "    Z_norm = torch.linalg.norm(Z, dim=1, keepdim=True)  # Size (n, 1).\n",
    "    B_norm = torch.linalg.norm(B, dim=0, keepdim=True)  # Size (1, b).\n",
    "    cosine_similarity = ((Z @ B) / (Z_norm @ B_norm)).T\n",
    "    return cosine_similarity\n",
    "\n",
    "def get_top_retrievals(clipvoxel, all_images, total_retrievals = 1):\n",
    "    '''\n",
    "    clipvoxel: output from do_recons that contains that information needed for retrievals\n",
    "    all_images: all ground truth actually seen images by the participant in day 2 run 1\n",
    "\n",
    "    outputs the top retrievals\n",
    "    '''\n",
    "    values_dict = {}\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        emb = clip_img_embedder(torch.reshape(all_images,(all_images.shape[0], 3, 224, 224)).to(device)).float() # CLIP-Image\n",
    "        emb = emb.cpu()\n",
    "        emb_ = clipvoxel # CLIP-Brain\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = np.reshape(emb_, (1, 425984))\n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "        emb_ = emb_.float()\n",
    "        fwd_sim = batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "        print(\"Given Brain embedding, find correct Image embedding\")\n",
    "    fwd_sim = np.array(fwd_sim.cpu())\n",
    "    which = np.flip(np.argsort(fwd_sim, axis = 0))\n",
    "    imsize = 224\n",
    "    for attempt in range(total_retrievals):\n",
    "        values_dict[f\"attempt{(attempt+1)}\"] = transforms.Resize((imsize,imsize), antialias=True)(all_images[which[attempt].copy()]).float().numpy().tolist()\n",
    "    return values_dict\n",
    "\n",
    "\n",
    "def convert_image_array_to_PIL(image_array):\n",
    "    if image_array.ndim == 4:\n",
    "        image_array = image_array[0]\n",
    "\n",
    "    # get the dimension to h, w, 3|1\n",
    "    if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "    \n",
    "    # clip the image array to 0-1\n",
    "    image_array = np.clip(image_array, 0, 1)\n",
    "    # convert the image array to uint8\n",
    "    image_array = (image_array * 255).astype('uint8')\n",
    "    # convert the image array to PIL\n",
    "    return Image.fromarray(image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9579fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "# load fmriprepped data\n",
    "fmriprep_data_path = '/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/testing'\n",
    "fmriprep_data = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    tmp = nib.load(f'{fmriprep_data_path}/sub-005_ses-03_task-C_run-{run+1:02d}_space-T1w_desc-preproc_bold.nii.gz')\n",
    "    fmriprep_data.append(tmp)\n",
    "\n",
    "fmriprep_data = np.array(fmriprep_data)\n",
    "print(fmriprep_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24be46fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m191\u001b[39m):\n\u001b[1;32m      6\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(mc_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mses-03_run-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mc_boldres.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43mtmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tmp\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m76\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m74\u001b[39m)\n\u001b[1;32m      9\u001b[0m     mc_vols[i, tr] \u001b[38;5;241m=\u001b[39m tmp\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/nibabel/dataobj_images.py:373\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[0;34m(self, caching, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/nibabel/arrayproxy.py:457\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/nibabel/arrayproxy.py:424\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    422\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_unscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m)\u001b[49m, scl_slope, scl_inter)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/nibabel/arrayproxy.py:394\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[1;32m    391\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    392\u001b[0m ):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    404\u001b[0m         fileobj,\n\u001b[1;32m    405\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[1;32m    411\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/nibabel/volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(infile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    464\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n_bytes)\n\u001b[0;32m--> 465\u001b[0m     n_read \u001b[38;5;241m=\u001b[39m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/rt_mindEye2/lib/python3.11/gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mc_dir = '/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/derivatives/motion_corrected_resampled'\n",
    "\n",
    "mc_vols = np.zeros(shape=(n_runs, 191, 76, 90, 74))\n",
    "for i in range(n_runs):\n",
    "    for tr in range(191):\n",
    "        tmp = nib.load(os.path.join(mc_dir, f'ses-03_run-{i+1:02d}_{tr:04d}_mc_boldres.nii.gz'))\n",
    "        tmp = tmp.get_fdata()\n",
    "        assert tmp.shape == (76, 90, 74)\n",
    "        mc_vols[i, tr] = tmp\n",
    "\n",
    "mc_data = [nib.Nifti1Image(mc_vols[i].transpose(1,2,3,0), affine=union_mask_img.affine) for i in range(n_runs)]\n",
    "mc_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff1807bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images=True\n",
    "save_individual_images=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6037c29",
   "metadata": {},
   "source": [
    "#### real-time compatible preprocessing, fit GLM with data up to each TR only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nilearn firstlevel glm\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "all_betas = []\n",
    "for run in range(1, n_runs+1):\n",
    "    save_path = f\"{derivatives_path}/sub-005_ses-03_task-C_run-{run:02d}_recons_data-fmriprep-all_delay-8\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_individual_images = True\n",
    "    if save_individual_images:\n",
    "        os.makedirs(os.path.join(save_path, \"individual_images\"), exist_ok=True)\n",
    "\n",
    "    all_recons_save = []\n",
    "    all_clipvoxels_save = []\n",
    "    all_ground_truth_save = []\n",
    "    all_retrieved_save = []\n",
    "\n",
    "\n",
    "    print(f\"Run {run} started\")\n",
    "    events_df = ndscore_events[run-1]\n",
    "    tr_labels_hrf = ndscore_tr_labels[run-1][\"tr_label_hrf\"].tolist()\n",
    "    events_df = events_df[events_df['image_name'] != 'blank.jpg']  # must drop blank.jpg after tr_labels_hrf is defined to keep indexing consistent\n",
    "\n",
    "    for TR in range(192):\n",
    "        stimulus_trial_counter = np.sum(\n",
    "            [label != 'blank' for label in tr_labels_hrf[:TR]]\n",
    "        )  # stimulus_trial_counter is the trial number minus blank trials; this indexes cropped_events which does not contain blank.jpg since we don't model blank trials\n",
    "        print(f\"TR {TR}\")\n",
    "        current_label = tr_labels_hrf[TR]\n",
    "        print(current_label)\n",
    "        \n",
    "        if current_label not in ('blank', 'blank.jpg'):\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] = events_df['onset'].astype(float)\n",
    "\n",
    "            run_start_time = events_df['onset'].iloc[0]\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] -= run_start_time\n",
    "\n",
    "            cropped_events = events_df[events_df.onset <= TR*tr_length]\n",
    "            cropped_events = cropped_events.copy()\n",
    "            print(stimulus_trial_counter)\n",
    "            print(np.where(cropped_events['trial_number'] == stimulus_trial_counter))\n",
    "            assert cropped_events['trial_number'].eq(stimulus_trial_counter).any(), \"No matching trial_number found in cropped_events\"\n",
    "            cropped_events.loc[:, 'trial_type'] = np.where(cropped_events['trial_number'] == stimulus_trial_counter, \"probe\", \"reference\")\n",
    "            cropped_events = cropped_events.drop(columns=['is_correct', 'image_name', 'response_time', 'trial_number'])\n",
    "\n",
    "            lss_glm = FirstLevelModel(t_r=tr_length,slice_time_ref=0,hrf_model='glover',\n",
    "                        drift_model='cosine', drift_order=1,high_pass=0.01,mask_img=union_mask_img,\n",
    "                        signal_scaling=False,smoothing_fwhm=None,noise_model='ar1',\n",
    "                        n_jobs=-1,verbose=-1,memory_level=1,minimize_memory=True)\n",
    "\n",
    "            # lss_glm.fit(run_imgs=mc_data[run-1].slicer[:,:,:,:TR], events=cropped_events)\n",
    "            # lss_glm.fit(run_imgs=mc_data[run-1], events=cropped_events)\n",
    "            # lss_glm.fit(run_imgs=fmriprep_data[run-1].slicer[:,:,:,:TR], events=cropped_events)\n",
    "            lss_glm.fit(run_imgs=fmriprep_data[run-1], events=cropped_events)\n",
    "\n",
    "            dm = lss_glm.design_matrices_[0]\n",
    "            beta_map = lss_glm.compute_contrast(\"probe\", output_type=\"effect_size\")\n",
    "            beta_map_np = beta_map.get_fdata()\n",
    "            beta_map_np = fast_apply_mask(target=beta_map_np,mask=union_mask_img.get_fdata())[0]\n",
    "            all_betas.append(beta_map_np)\n",
    "\n",
    "            if \"MST_pairs\" in current_label:\n",
    "                correct_image_index = np.where(current_label == vox_image_names)[0][0]  # using the first occurrence based on image name, assumes that repeated images are identical (which they should be)\n",
    "                z_mean = np.mean(np.array(all_betas), axis=0)\n",
    "                z_std = np.std(np.array(all_betas), axis=0)\n",
    "                betas = ((np.array(all_betas) - z_mean) / (z_std + 1e-6))[-1]  # use only the beta pattern from the most recent image\n",
    "                # betas = np.array(all_betas)\n",
    "                # pdb.set_trace()\n",
    "                betas = betas[np.newaxis, np.newaxis, :]\n",
    "                betas_tt = torch.Tensor(betas).to(\"cpu\")\n",
    "                reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "                if clipvoxelsTR is None:\n",
    "                    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                        voxel = betas_tt\n",
    "                        voxel = voxel.to(device)\n",
    "                        assert voxel.shape[1] == 1\n",
    "                        voxel_ridge = model.ridge(voxel[:,[-1]],0) # 0th index of subj_list\n",
    "                        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                        blurry_image_enc = blurry_image_enc0[0]\n",
    "                        clipvoxelsTR = clip_voxels.cpu()\n",
    "                values_dict = get_top_retrievals(clipvoxelsTR, all_images=images[MST_idx], total_retrievals=5)\n",
    "                image_array = np.array(reconsTR)[0]\n",
    "                # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "                if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                    image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "                # Display the image\n",
    "                if plot_images:\n",
    "                    # plot original and reconstructed images\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.title(\"Reconstructed Image\")\n",
    "                    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot original with top 5 retrievals\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 6, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    for i in range(5):\n",
    "                        plt.subplot(1, 6, i+2)\n",
    "                        plt.title(f\"Retrieval {i+1}\")\n",
    "                        plt.imshow(np.array(values_dict[f\"attempt{i+1}\"][0]).transpose(1, 2, 0), cmap='gray')\n",
    "                        plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    if save_individual_images:\n",
    "                    # save the reconstructed image\n",
    "                        convert_image_array_to_PIL(image_array).save(os.path.join(save_path, \"individual_images\", f\"run{run}_TR{TR}_reconstructed.png\"))\n",
    "                        # save the retrieved images\n",
    "                        for key, value in values_dict.items():\n",
    "                            if (not ('ground_truth' in key)):\n",
    "                                convert_image_array_to_PIL(np.array(value)).save(os.path.join(save_path, \"individual_images\", f\"run{run}_TR{TR}_retrieved_{key}.png\"))\n",
    "                        # save the clip_voxels\n",
    "                        np.save(os.path.join(save_path, \"individual_images\", f\"run{run}_TR{TR}_clip_voxels.npy\"), clipvoxelsTR)\n",
    "                        # save the ground truth image\n",
    "                        convert_image_array_to_PIL(images[correct_image_index].numpy()).save(os.path.join(save_path, \"individual_images\", f\"run{run}_TR{TR}_ground_truth.png\"))\n",
    "                        all_recons_save.append(image_array)\n",
    "                        all_clipvoxels_save.append(clipvoxelsTR)\n",
    "                        all_ground_truth_save.append(images[correct_image_index].numpy())\n",
    "                        all_retrieved_save.append([np.array(value) for key, value in values_dict.items() if (not ('ground_truth' in key))])\n",
    "\n",
    "                    elif current_label == 'blank.jpg':\n",
    "                        pass\n",
    "                    else:\n",
    "                        assert current_label == 'blank'\n",
    "\n",
    "    # save the design matrix for the current run\n",
    "    dm.to_csv(os.path.join(save_path, f\"design_run-{run:02d}.csv\"))\n",
    "    plot_design_matrix(dm, output_file=os.path.join(save_path, \"dm\"))\n",
    "    dm[['probe', 'reference']].plot(title='Probe/Reference Regressors', figsize=(10, 4))\n",
    "    plt.savefig(os.path.join(save_path, \"regressors\"))\n",
    "    # save betas so far\n",
    "    np.save(os.path.join(save_path, f\"betas_run-{run:02d}.npy\"), np.array(all_betas))\n",
    "    print(f\"==END OF RUN {run}!==\\n\")\n",
    "    # save the tensors\n",
    "    if all_recons_save:\n",
    "        all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "        all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "        all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "        all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "        torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "        torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "        torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "        torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "        print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "        print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "        print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "        print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "        print(\"All tensors saved successfully on \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f\"{derivatives_path}/debugging/betas_run-01-11_data-fmriprep-all_delay-8.npy\", np.array(all_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7298851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b1 = np.load('/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/derivatives/debugging/betas_run-01-11_data-fmriprep-all_delay-8.npy')\n",
    "b2 = np.array(all_betas)\n",
    "betas_path = f'{data_path}/testing/sub-005_ses-03_task-C_bs24_MST_rishab_MSTsplit_union_mask_0_vox.npy'\n",
    "offline_betas_norm = np.load(betas_path)\n",
    "\n",
    "# corrs, p = utils_mindeye.vectorized_pearsonr(b2[:63], offline_betas_norm[:63])\n",
    "corrs, p = utils_mindeye.vectorized_pearsonr(b1, offline_betas_norm)\n",
    "plt.hist(corrs)\n",
    "plt.title('Correlation between fmriprep+nilearn (delay=none) and fmriprep+glmsingle betas (11 runs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a4d40-643d-493b-874b-2030490b9bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mc_dir = f\"{derivatives_path}/motion_corrected\"\n",
    "mc_resampled_dir = f\"{derivatives_path}/motion_corrected_resampled\"\n",
    "if os.path.exists(mc_dir):\n",
    "    shutil.rmtree(mc_dir)\n",
    "os.makedirs(mc_dir)\n",
    "if os.path.exists(mc_resampled_dir):\n",
    "    shutil.rmtree(mc_resampled_dir)\n",
    "os.makedirs(mc_resampled_dir)\n",
    "\n",
    "# set the output type to NIFTI_GZ\n",
    "os.environ['FSLOUTPUTTYPE'] = 'NIFTI_GZ'\n",
    "assert np.all(ses1_boldref_nib.affine == union_mask_img.affine)\n",
    "all_betas = []\n",
    "\n",
    "# Loop over all 11 runs in the session\n",
    "n_runs = 11\n",
    "for run_num in range(1, n_runs + 1):\n",
    "    print(f\"Run {run_num} started\")\n",
    "    mc_params = []\n",
    "    imgs = []\n",
    "    events_df = ndscore_events[run_num - 1]\n",
    "    tr_labels_hrf = ndscore_tr_labels[run_num - 1][\"tr_label_hrf\"].tolist()\n",
    "    events_df = events_df[events_df['image_name'] != 'blank.jpg']  # must drop blank.jpg after tr_labels_hrf is defined to keep indexing consistent\n",
    "    beta_maps_list = []\n",
    "    all_trial_names_list = []\n",
    "    all_images = None\n",
    "    \n",
    "    # seen_label_before = [\"blank\"]\n",
    "    # get the list of all images in torch tensor format for this run (should be 62 or 63 images)\n",
    "    # all_COCO_ids = []\n",
    "    # for TR in range(192):\n",
    "    #     if tr_labels_hrf[TR] not in seen_label_before:\n",
    "    #         seen_label_before.append(tr_labels_hrf[TR])\n",
    "    #         image_id = np.where(tr_labels_hrf[TR] == vox_image_names)[0]\n",
    "    #         if len(image_id) > 0:\n",
    "    #             # check images are identical\n",
    "    #             for i in range(len(image_id)):    \n",
    "    #                 assert torch.all(images[i] == images[image_id[i]])\n",
    "    #         new_image_pt = images[image_id[0][0]]\n",
    "    #         all_images = new_image_pt if all_images == None else torch.vstack((all_images, new_image_pt))\n",
    "    #         all_COCO_ids.append(image_id)\n",
    "    # print(all_COCO_ids)\n",
    "\n",
    "\n",
    "    # define save_path\n",
    "    save_path = f\"{derivatives_path}/sub-005_ses-03_task-C_run-{run_num:02d}_recons\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_individual_images = True\n",
    "    if save_individual_images:\n",
    "        os.makedirs(os.path.join(save_path, \"individual_images\"), exist_ok=True)\n",
    "\n",
    "    all_recons_save = []\n",
    "    all_clipvoxels_save = []\n",
    "    all_ground_truth_save = []\n",
    "    all_retrieved_save = []\n",
    "\n",
    "    stimulus_trial_counter = 0\n",
    "    plot_images = True\n",
    "    T1_brain = f\"{data_path}/sub-005_desc-preproc_T1w_brain.nii.gz\"\n",
    "    n_trs = 192\n",
    "    assert len(tr_labels_hrf) == n_trs, \"there should be image labels for each TR\"\n",
    "    assert all(label in image_names for label in tr_labels_hrf if label != 'blank'), \"Some labels in tr_labels_hrf are missing from image_names.\"\n",
    "    assert len(images) > n_trs, \"images array is too short.\"\n",
    "    for TR in range(n_trs-1):\n",
    "        print(f\"TR {TR}\")\n",
    "        # stream in the nifti\n",
    "        cur_vol = f\"sub-005_ses-03_task-C_run-{run_num:02d}_bold_{TR:04d}\"\n",
    "        image_data = nib.load(f\"{derivatives_path}/vols/{cur_vol}.nii.gz\")\n",
    "        current_label = tr_labels_hrf[TR]\n",
    "        print(current_label)\n",
    "        if TR == 0 and run_num == 1:\n",
    "            ses3_vol0 = image_data\n",
    "            # make the day 2 bold ref\n",
    "            # nib.save(image_data, day2_boldref)\n",
    "            # save the transformation from the day 2 bold ref to the day 1 \n",
    "            # os.system(f\"antsRegistrationSyNQuick.sh \\\n",
    "            #   -d 3 \\\n",
    "            #   -f {T1_brain} \\\n",
    "            #   -m {ses3_vol0} \\\n",
    "            #   -o {derivatives_path}/ses3_vol0_epi2T1_\")\n",
    "\n",
    "            # for simulation, just load it in\n",
    "            ses3_boldref_path = f\"{derivatives_path}/ses3_vol0_epi2T1_Warped.nii.gz\"\n",
    "            ses3_boldref = nib.load(ses3_boldref_path)\n",
    "        # load nifti file\n",
    "        # tmp = f'{storage_path}/day2_subj1/tmp_run{run_num}.nii.gz'\n",
    "        # nib.save(index_img(image_data,0),tmp)\n",
    "        start = time.time()\n",
    "        mc = f\"{mc_dir}/{cur_vol}_mc\"\n",
    "        os.system(f\"{fsl_path}/mcflirt -in {derivatives_path}/vols/{cur_vol}.nii.gz -reffile {derivatives_path}/vols/sub-005_ses-03_task-C_run-01_bold_0000.nii.gz -out {mc} -plots -mats\")\n",
    "        mc_params.append(np.loadtxt(f'{mc}.par'))\n",
    "\n",
    "        final_vol = f\"{mc_resampled_dir}/ses-03_run-{run_num:02d}_{TR:04d}_mc_boldres.nii.gz\"\n",
    "        # use ants apply_transforms to chain transformations from current volume to boldref and boldref to T1\n",
    "        warped_img = ants.apply_transforms(\n",
    "            fixed=ants.image_read(ses1_boldref),\n",
    "            moving=ants.image_read(f\"{mc}.nii.gz\"),\n",
    "            transformlist=[\n",
    "                f\"{derivatives_path}/ses3_vol0_epi2T1_0GenericAffine.mat\",\n",
    "                f\"{derivatives_path}/ses3_vol0_epi2T1_1Warp.nii.gz\"\n",
    "            ],\n",
    "            interpolator='linear'  # or 'nearestNeighbor', 'bspline', etc.\n",
    "        )\n",
    "        ants.image_write(warped_img, final_vol)\n",
    "        imgs.append(warped_img.get_fdata()) # only add to imgs list\n",
    "        os.system(f\"rm -r {mc}.mat\")\n",
    "        \n",
    "        # if tr_labels_hrf[TR] != tr_labels_hrf[TR + 1] and tr_labels_hrf[TR] != \"blank\":\n",
    "        #     print('last TR of real image')\n",
    "        #     cropped_events = events_df[events_df.trial_number <= int(float(tr_labels_hrf[TR].split(\"_\")[3]))].astype(str)\n",
    "        #     for i_trial, trial in cropped_events.iterrows():\n",
    "        #         cropped_events.loc[i_trial, \"trial_type\"] = \"reference\" if i_trial < (len(cropped_events) - 1) else \"probe\"\n",
    "                \n",
    "        #     # cropped_events = cropped_events.drop(columns=['total_novel_presses', 'change_mind', 'is_correct', 'time', \n",
    "        #     #                                     'response_time', 'response', '73k_id', 'trial_number', \n",
    "        #     #                                     '10k_id', 'memory_first', 'is_old_session', 'is_correct_session', \n",
    "        #     #                                     'missing_data', 'total_old_presses', 'memory_recent'])  # nilearn glm fit requires onset and duration; trial_type is optional (https://nilearn.github.io/stable/modules/generated/nilearn.glm.first_level.make_first_level_design_matrix.html#nilearn.glm.first_level.make_first_level_design_matrix)\n",
    "\n",
    "        #     # get the image id from this stimulus trial that we are fitting a model on\n",
    "        #     image_COCO_id = int(float(tr_labels_hrf[TR].split(\"_\")[1])) - 1\n",
    "        if current_label not in ('blank', 'blank.jpg'):\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] = events_df['onset'].astype(float)\n",
    "\n",
    "            run_start_time = events_df['onset'].iloc[0]\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] -= run_start_time\n",
    "\n",
    "            cropped_events = events_df[events_df.onset <= TR*tr_length]\n",
    "            cropped_events = cropped_events.copy()\n",
    "            cropped_events.loc[:, 'trial_type'] = np.where(cropped_events['trial_number'] == stimulus_trial_counter, \"probe\", \"reference\")\n",
    "            cropped_events = cropped_events.drop(columns=['is_correct', 'image_name', 'response_time', 'trial_number'])\n",
    "\n",
    "            # collect all of the images at each TR into a 4D time series\n",
    "            img = np.rollaxis(np.array(imgs),0,4)\n",
    "            img = new_img_like(ses1_boldref_nib,img,copy_header=True)\n",
    "            # run the model with mc_params confounds to motion correct\n",
    "            lss_glm = FirstLevelModel(t_r=tr_length,slice_time_ref=0,hrf_model='glover',\n",
    "                        drift_model='cosine', drift_order=1,high_pass=0.01,mask_img=union_mask_img,\n",
    "                        signal_scaling=False,smoothing_fwhm=None,noise_model='ar1',\n",
    "                        n_jobs=-1,verbose=-1,memory_level=1,minimize_memory=True)\n",
    "            \n",
    "            lss_glm.fit(run_imgs=img, events=cropped_events, confounds = pd.DataFrame(np.array(mc_params)))\n",
    "            dm = lss_glm.design_matrices_[0]\n",
    "            # get the beta map and mask it\n",
    "            beta_map = lss_glm.compute_contrast(\"probe\", output_type=\"effect_size\")\n",
    "            beta_map_np = beta_map.get_fdata()\n",
    "            beta_map_np = fast_apply_mask(target=beta_map_np,mask=union_mask_img.get_fdata())[0]\n",
    "            # beta_map_np = np.reshape(beta_map_np, (1,1,19174))\n",
    "            # pdb.set_trace()\n",
    "            # beta_map_np = beta_map_np[0,0,union_mask]\n",
    "            all_betas.append(beta_map_np)\n",
    "            if \"MST_pairs\" in current_label:\n",
    "                correct_image_index = np.where(current_label == vox_image_names)[0][0]  # using the first occurrence based on image name, assumes that repeated images are identical (which they should be)\n",
    "                # z-score the beta map\n",
    "                # if run_num == 1:\n",
    "                #     # use avg of ses-01 and 02 (ses-01: -0.11676685 ± 1.0246168; ses-02: -0.02013384 ± 0.98079187)\n",
    "                #     z_mean = -0.0684\n",
    "                #     z_std = 1.0027\n",
    "                # else:\n",
    "                z_mean = np.mean(np.array(all_betas), axis=0)\n",
    "                z_std = np.std(np.array(all_betas), axis=0)\n",
    "                betas = ((np.array(all_betas) - z_mean) / (z_std + 1e-6))[-1]  # use only the beta pattern from the most recent image\n",
    "                # betas = np.array(all_betas)\n",
    "                # pdb.set_trace()\n",
    "                betas = betas[np.newaxis, np.newaxis, :]\n",
    "                betas_tt = torch.Tensor(betas).to(\"cpu\")\n",
    "                reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "                if clipvoxelsTR is None:\n",
    "                    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                        voxel = betas_tt\n",
    "                        voxel = voxel.to(device)\n",
    "                        assert voxel.shape[1] == 1\n",
    "                        voxel_ridge = model.ridge(voxel[:,[-1]],0) # 0th index of subj_list\n",
    "                        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                        blurry_image_enc = blurry_image_enc0[0]\n",
    "                        clipvoxelsTR = clip_voxels.cpu()\n",
    "                values_dict = get_top_retrievals(clipvoxelsTR, all_images=images[MST_idx], total_retrievals=5)\n",
    "                image_array = np.array(reconsTR)[0]\n",
    "                # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "                if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                    image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "                # Display the image\n",
    "                if plot_images:\n",
    "                    # plot original and reconstructed images\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.title(\"Reconstructed Image\")\n",
    "                    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot original with top 5 retrievals\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 6, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    for i in range(5):\n",
    "                        plt.subplot(1, 6, i+2)\n",
    "                        plt.title(f\"Retrieval {i+1}\")\n",
    "                        plt.imshow(np.array(values_dict[f\"attempt{i+1}\"][0]).transpose(1, 2, 0), cmap='gray')\n",
    "                        plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                # subjInterface.setResultDict allows us to send to the analysis listener immediately\n",
    "                # subjInterface.setResultDict(name=f'run{run_num}_TR{TR}',\n",
    "                #                             values=values_dict)            \n",
    "                # save reconstructed image, retrieved images, clip_voxels, and ground truth image\n",
    "                if save_individual_images:\n",
    "                    # save the reconstructed image\n",
    "                    convert_image_array_to_PIL(image_array).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_reconstructed.png\"))\n",
    "                    # save the retrieved images\n",
    "                    for key, value in values_dict.items():\n",
    "                        if (not ('ground_truth' in key)):\n",
    "                            convert_image_array_to_PIL(np.array(value)).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_retrieved_{key}.png\"))\n",
    "                    # save the clip_voxels\n",
    "                    np.save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_clip_voxels.npy\"), clipvoxelsTR)\n",
    "                    # save the ground truth image\n",
    "                    convert_image_array_to_PIL(images[correct_image_index].numpy()).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_ground_truth.png\"))\n",
    "                all_recons_save.append(image_array)\n",
    "                all_clipvoxels_save.append(clipvoxelsTR)\n",
    "                all_ground_truth_save.append(images[correct_image_index].numpy())\n",
    "                all_retrieved_save.append([np.array(value) for key, value in values_dict.items() if (not ('ground_truth' in key))])\n",
    "            else:\n",
    "                pass\n",
    "                # imsize = 224\n",
    "                # values_dict[\"ground_truth\"] = transforms.Resize((imsize,imsize), antialias=True)(images[stimulus_trial_counter]).float().numpy().tolist()\n",
    "                # image_array = np.array(values_dict[\"ground_truth\"])\n",
    "\n",
    "                # # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "                # if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                #     image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "                # # Display the image\n",
    "                # if plot_images:\n",
    "                #     plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                #     plt.axis('off')  # Hide axes\n",
    "                #     plt.show()\n",
    "            stimulus_trial_counter += 1\n",
    "        elif current_label == 'blank.jpg':\n",
    "            stimulus_trial_counter += 1\n",
    "        else:\n",
    "            assert current_label == 'blank'\n",
    "    # save the design matrix for the current run\n",
    "    dm.to_csv(os.path.join(save_path, f\"design_run-{run_num:02d}.csv\"))\n",
    "    plot_design_matrix(dm, output_file=os.path.join(save_path, \"dm\"))\n",
    "    dm[['probe', 'reference']].plot(title='Probe/Reference Regressors', figsize=(10, 4))\n",
    "    plt.savefig(os.path.join(save_path, \"regressors\"))\n",
    "    # save betas so far\n",
    "    np.save(os.path.join(save_path, f\"betas_run-{run_num:02d}.npy\"), np.array(all_betas))\n",
    "    print(f\"==END OF RUN {run_num}!==\\n\")\n",
    "    # save the tensors\n",
    "    if all_recons_save:\n",
    "        all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "        all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "        all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "        all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "        torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "        torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "        torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "        torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "        print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "        print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "        print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "        print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "        print(\"All tensors saved successfully on \", save_path)\n",
    "\n",
    "    # except:\n",
    "    #     pass\n",
    "        # save the tensors\n",
    "        # all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "        # all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "        # all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "        # all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "        # torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "        # torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "        # torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "        # torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "        # print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "        # print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "        # print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "        # print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "        # print(\"All tensors saved successfully on \", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec54d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and display all recons with originals\n",
    "# all_recons_save = torch.load(os.path.join(save_path, \"all_recons.pt\"))\n",
    "# for i in range(len(all_recons_save)):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.imshow(all_ground_truth_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.title(\"Reconstructed Image\")\n",
    "#     plt.imshow(all_recons_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and display top 5 retrievals and originals\n",
    "# for i in range(len(all_retrieved_save)):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.subplot(1, 6, 1)\n",
    "#     plt.title(\"Original\")\n",
    "#     plt.imshow(all_ground_truth_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "#     for j in range(5):\n",
    "#         plt.subplot(1, 6, j+2)\n",
    "#         plt.title(f\"Top {j+1}\")\n",
    "#         plt.imshow(all_retrieved_save[i][j][0].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#         plt.axis('off')  # Hide axes\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tr_labels_hrf[TR] != tr_labels_hrf[TR + 1] and tr_labels_hrf[TR] != \"blank\":\n",
    "#     print('last TR of real image')\n",
    "#     cropped_events = events_df[events_df.trial_number <= int(float(tr_labels_hrf[TR].split(\"_\")[3]))].astype(str)\n",
    "#     for i_trial, trial in cropped_events.iterrows():\n",
    "#         cropped_events.loc[i_trial, \"trial_type\"] = \"reference\" if i_trial < (len(cropped_events) - 1) else \"probe\"\n",
    "\n",
    "#     # Map the TR label to the corresponding image index\n",
    "#     current_label = tr_labels_hrf[TR]\n",
    "#     try:\n",
    "#         # Find the index of the image in vox_image_names\n",
    "#         image_index = np.where(vox_image_names == current_label)[0][0]\n",
    "#         # Retrieve the corresponding image from the images tensor\n",
    "#         current_image = images[image_index]\n",
    "#     except IndexError:\n",
    "#         print(f\"Warning: Label {current_label} not found in vox_image_names. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     # Collect all of the images at each TR into a 4D time series\n",
    "#     img = np.rollaxis(np.array(imgs), 0, 4)\n",
    "#     img = new_img_like(ses1_boldref_nib, img, copy_header=True)\n",
    "\n",
    "#     # Run the model with mc_params confounds to motion correct\n",
    "#     lss_glm.fit(run_imgs=img, events=cropped_events, confounds=pd.DataFrame(np.array(mc_params)))\n",
    "\n",
    "#     # Get the beta map and mask it\n",
    "#     beta_map = lss_glm.compute_contrast(\"probe\", output_type=\"effect_size\")\n",
    "#     beta_map_np = beta_map.get_fdata()\n",
    "#     beta_map_np = fast_apply_mask(target=beta_map_np, mask=mask_img.get_fdata())\n",
    "#     beta_map_np = np.reshape(beta_map_np, (1, 1, 25225))\n",
    "#     betas_tt = torch.Tensor(beta_map_np).to(\"cpu\")\n",
    "\n",
    "#     # Use the current image for further processing\n",
    "#     reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "#     values_dict = get_top_retrievals(clipvoxelsTR, all_images=images, stimulus_trial_counter=image_index, total_retrievals=5)\n",
    "#     image_array = np.array(reconsTR)[0]\n",
    "\n",
    "#     # If the image has 3 channels (RGB), reorder the dimensions\n",
    "#     if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "#         image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "#     # Display the image\n",
    "#     if plot_images:\n",
    "#         plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "#         plt.axis('off')  # Hide axes\n",
    "#         plt.show()\n",
    "\n",
    "#     # Save reconstructed image, retrieved images, clip_voxels, and ground truth image\n",
    "#     if save_individual_images:\n",
    "#         convert_image_array_to_PIL(image_array).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_reconstructed.png\"))\n",
    "#         for key, value in values_dict.items():\n",
    "#             if not ('ground_truth' in key):\n",
    "#                 convert_image_array_to_PIL(np.array(value)).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_retrieved_{key}.png\"))\n",
    "#         np.save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_clip_voxels.npy\"), clipvoxelsTR)\n",
    "#         convert_image_array_to_PIL(np.array(values_dict[\"ground_truth\"])).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_ground_truth.png\"))\n",
    "\n",
    "#     all_recons_save.append(image_array)\n",
    "#     all_clipvoxels_save.append(clipvoxelsTR)\n",
    "#     all_ground_truth_save.append(np.array(values_dict[\"ground_truth\"]))\n",
    "#     all_retrieved_save.append([np.array(value) for key, value in values_dict.items() if not ('ground_truth' in key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_recons_and_evaluate(\n",
    "    beta_series,  # shape: (n_conditions, n_voxels)\n",
    "    images,       # tensor of all images, shape: (n_images, 3, 224, 224)\n",
    "    model,        # loaded model\n",
    "    do_reconstructions,  # function for recon\n",
    "    device,       # torch device\n",
    "    save_path=None,    # directory to save results\n",
    "    metrics_module=None,  # module or dict with metric functions, optional\n",
    "    save_results=False,   # flag to control saving, defaults to False\n",
    "    test_idx=None,         # indices for test set, optional\n",
    "    do_zscore=False\n",
    "):\n",
    "    if test_idx is not None:\n",
    "        beta_series = beta_series[test_idx]\n",
    "        images = images[test_idx]\n",
    "\n",
    "    all_recons_save_tensor = []\n",
    "    all_clipvoxels_save_tensor = []\n",
    "    all_ground_truth_save_tensor = []\n",
    "\n",
    "    for idx in range(beta_series.shape[0]):\n",
    "        beta_pattern = beta_series[np.newaxis, np.newaxis, idx, :]  # (1,1,n_vox)\n",
    "        if do_zscore:\n",
    "            # Z-score using only betas up to and including the current image\n",
    "            beta_series_up_to_now = beta_series[:idx+1]\n",
    "            beta_pattern = utils_mindeye.zscore(beta_pattern, beta_series_up_to_now)\n",
    "\n",
    "        betas_tt = torch.Tensor(beta_pattern).to(\"cpu\")\n",
    "        reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "        if clipvoxelsTR is None:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                voxel = betas_tt.to(device)\n",
    "                assert voxel.shape[1] == 1\n",
    "                voxel_ridge = model.ridge(voxel[:, [-1]], 0)\n",
    "                backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "                clipvoxelsTR = clip_voxels0.cpu()\n",
    "\n",
    "        image_array = np.array(reconsTR)[0]\n",
    "        if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "            image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "        all_recons_save_tensor.append(reconsTR)\n",
    "        all_clipvoxels_save_tensor.append(clipvoxelsTR)\n",
    "        all_ground_truth_save_tensor.append(images[idx])\n",
    "\n",
    "    all_recons_save_tensor = torch.stack(all_recons_save_tensor)\n",
    "    all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save_tensor)\n",
    "    all_ground_truth_save_tensor = torch.stack(all_ground_truth_save_tensor)\n",
    "\n",
    "    if save_results and save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "        torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "        torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "\n",
    "    if metrics_module is not None:\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            unique_clip_voxels = all_clipvoxels_save_tensor\n",
    "            unique_ground_truth = all_ground_truth_save_tensor\n",
    "            all_fwd_acc, all_bwd_acc = metrics_module['calculate_retrieval_metrics'](unique_clip_voxels, unique_ground_truth)\n",
    "            pixcorr = metrics_module['calculate_pixcorr'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "            ssim_ = metrics_module['calculate_ssim'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "            alexnet2, alexnet5 = metrics_module['calculate_alexnet'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "            inception = metrics_module['calculate_inception_v3'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "            clip_ = metrics_module['calculate_clip'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "            efficientnet = metrics_module['calculate_efficientnet_b1'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "            swav = metrics_module['calculate_swav'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "        df_metrics = pd.DataFrame({\n",
    "            \"Metric\": [\n",
    "                \"alexnet2\",\n",
    "                \"alexnet5\",\n",
    "                \"inception\",\n",
    "                \"clip_\",\n",
    "                \"efficientnet\",\n",
    "                \"swav\",\n",
    "                \"pixcorr\",\n",
    "                \"ssim\",\n",
    "                \"all_fwd_acc\",\n",
    "                \"all_bwd_acc\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                alexnet2,\n",
    "                alexnet5,\n",
    "                inception,\n",
    "                clip_,\n",
    "                efficientnet,\n",
    "                swav,\n",
    "                pixcorr,\n",
    "                ssim_,\n",
    "                all_fwd_acc,\n",
    "                all_bwd_acc\n",
    "            ]\n",
    "        })\n",
    "        df_metrics.set_index(\"Metric\", inplace=True)\n",
    "        if save_results and save_path is not None:\n",
    "            df_metrics.to_csv(os.path.join(save_path, \"metrics.csv\"))\n",
    "        print(df_metrics)\n",
    "\n",
    "    return all_recons_save_tensor, all_clipvoxels_save_tensor, all_ground_truth_save_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c292df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmriprep_betas_slicer_hrf6 = np.load('/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/derivatives/debugging/betas_run-01_data-fmriprep-slicer.npy')\n",
    "run_recons_and_evaluate(fmriprep_betas_slicer_hrf6, images, model, do_reconstructions, device, test_idx=MST_idx[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6675825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the first repeat only:  [(1, 5), (0, 12), (14, 18), (8, 21), (2, 37), (11, 38), (29, 42), (31, 44), (33, 52), (9, 56), (34, 58), (39, 61), (30, 62), (20, 63), (48, 64), (10, 66), (7, 69), (41, 72), (70, 73), (19, 74), (3, 75), (23, 76), (43, 77), (22, 78), (51, 79), (28, 80), (67, 81), (65, 83), (32, 84), (68, 85), (4, 86), (53, 87), (13, 89), (15, 90), (27, 91), (26, 92), (17, 94), (59, 95), (6, 99), (25, 100), (35, 101), (82, 102), (50, 103), (46, 104), (40, 105), (97, 106), (60, 107), (57, 108), (16, 109), (54, 110), (98, 112), (96, 113), (36, 114), (88, 115), (24, 116), (47, 117), (49, 118), (93, 119), (111, 120), (55, 121), (45, 122), (71, 123)]\n",
      "Loading clip_img_embedder\n",
      "The total pool of images and clip voxels to do retrieval on is:  62\n",
      "Creating embeddings for images\n",
      "Calculating retrieval metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall fwd percent_correct: 0.3065\n",
      "overall bwd percent_correct: 0.3871\n",
      "torch.Size([124, 541875])\n",
      "torch.Size([124, 541875])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 260.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Correlation: 0.11223332856947231\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:01<00:00, 102.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.35683707400539755\n",
      "Loading AlexNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---early, AlexNet(2)---\n",
      "2-way Percent Correct (early AlexNet): 0.6855\n",
      "\n",
      "---mid, AlexNet(5)---\n",
      "2-way Percent Correct (mid AlexNet): 0.6991\n",
      "Loading Inception V3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/models/feature_extraction.py:174: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
      "  warnings.warn(msg + suggestion_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-way Percent Correct (Inception V3): 0.5636\n",
      "Loading CLIP\n",
      "2-way Percent Correct (CLIP): 0.5391\n",
      "Loading EfficientNet B1\n",
      "Distance EfficientNet B1: 0.9272306189678844\n",
      "Loading SwAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ri4541@pu.win.princeton.edu/.cache/torch/hub/facebookresearch_swav_main\n",
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ri4541@pu.win.princeton.edu/miniforge3/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance SwAV: 0.600111464234421\n",
      "                 Value\n",
      "Metric                \n",
      "alexnet2      0.685549\n",
      "alexnet5      0.699056\n",
      "inception     0.563598\n",
      "clip_         0.539077\n",
      "efficientnet  0.927231\n",
      "swav          0.600111\n",
      "pixcorr       0.112233\n",
      "ssim          0.356837\n",
      "all_fwd_acc   0.306452\n",
      "all_bwd_acc   0.387097\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation metrics\n",
    "from utils_mindeye import calculate_retrieval_metrics, calculate_alexnet, calculate_clip, calculate_swav, calculate_efficientnet_b1, calculate_inception_v3, calculate_pixcorr, calculate_ssim, deduplicate_tensors\n",
    "all_recons_save_tensor = []\n",
    "all_clipvoxels_save_tensor = []\n",
    "all_ground_truth_save_tensor = []\n",
    "all_retrieved_save_tensor = []\n",
    "\n",
    "for run_num in range(n_runs):\n",
    "    save_path = f\"{derivatives_path}/sub-005_ses-03_task-C_run-{run_num+1:02d}_recons_data-fmriprep-all_delay-8\"\n",
    "\n",
    "    try:\n",
    "        # recons = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16)\n",
    "        # clipvoxels = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16)\n",
    "        # ground_truth = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16)\n",
    "        recons = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16).to(device)\n",
    "        clipvoxels = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16).to(device)\n",
    "        ground_truth = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16).to(device)\n",
    "\n",
    "        all_recons_save_tensor.append(recons)\n",
    "        all_clipvoxels_save_tensor.append(clipvoxels)\n",
    "        all_ground_truth_save_tensor.append(ground_truth)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Tensors not found. Please check the save path.\")\n",
    "\n",
    "# Concatenate tensors along the first dimension\n",
    "try:\n",
    "    all_recons_save_tensor = torch.cat(all_recons_save_tensor, dim=0)\n",
    "    all_clipvoxels_save_tensor = torch.cat(all_clipvoxels_save_tensor, dim=0)\n",
    "    all_ground_truth_save_tensor = torch.cat(all_ground_truth_save_tensor, dim=0)\n",
    "except RuntimeError:\n",
    "    print('Error: Couldn\\'t concatenate tensors')\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    unique_clip_voxels, unique_ground_truth, duplicated = deduplicate_tensors(all_clipvoxels_save_tensor, all_ground_truth_save_tensor)\n",
    "    print(\"Using the first repeat only: \", duplicated)\n",
    "    unique_clip_voxels = all_clipvoxels_save_tensor[np.array(duplicated)[:,0]]\n",
    "    unique_ground_truth = all_ground_truth_save_tensor[np.array(duplicated)[:,0]]\n",
    "    all_fwd_acc, all_bwd_acc = calculate_retrieval_metrics(unique_clip_voxels, unique_ground_truth)\n",
    "    pixcorr = calculate_pixcorr(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    ssim_ = calculate_ssim(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    alexnet2, alexnet5 = calculate_alexnet(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    inception = calculate_inception_v3(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    clip_ = calculate_clip(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    efficientnet = calculate_efficientnet_b1(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    swav = calculate_swav(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "\n",
    "# save the results to a csv file\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"alexnet2\",\n",
    "        \"alexnet5\",\n",
    "        \"inception\",\n",
    "        \"clip_\",\n",
    "        \"efficientnet\",\n",
    "        \"swav\",\n",
    "        \"pixcorr\",\n",
    "        \"ssim\",\n",
    "        \"all_fwd_acc\",\n",
    "        \"all_bwd_acc\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        alexnet2,\n",
    "        alexnet5,\n",
    "        inception,\n",
    "        clip_,\n",
    "        efficientnet,\n",
    "        swav,\n",
    "        pixcorr,\n",
    "        ssim_,\n",
    "        all_fwd_acc,\n",
    "        all_bwd_acc\n",
    "    ]\n",
    "})\n",
    "df_metrics.set_index(\"Metric\", inplace=True)\n",
    "print(df_metrics)\n",
    "df_metrics.to_csv(os.path.join(save_path, \"metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed94795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs in tensor:\", torch.isnan(recons).any())\n",
    "print(\"Infs in tensor:\", torch.isinf(recons).any())\n",
    "print(\"Shape of tensor:\", recons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(10, 10).to(device)\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befaf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(duplicated)[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c557f",
   "metadata": {},
   "source": [
    "# Debugging Reconstructions and Retrievals\n",
    "This notebook tests the pipeline by using precomputed betas to perform top-5 retrievals and reconstructions. The goal is to confirm whether the pipeline produces correct results with these betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56109867",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas[np.newaxis, np.newaxis, i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8251c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_betas).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_path = f'{data_path}/testing/sub-005_ses-03_task-C_bs24_MST_rishab_MSTsplit_union_mask_0_vox.npy'\n",
    "betas = np.load(betas_path)\n",
    "\n",
    "# Iterate through the first 5 indices of the test set\n",
    "for idx in MST_idx[:5]:\n",
    "    beta_pattern = betas[np.newaxis, np.newaxis, idx, :]  # Add new axes as required\n",
    "    betas_tt = torch.Tensor(beta_pattern).to(\"cpu\")\n",
    "\n",
    "    # Perform reconstructions\n",
    "    reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "\n",
    "    if clipvoxelsTR is None:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            voxel = betas_tt.to(device)\n",
    "            assert voxel.shape[1] == 1\n",
    "            voxel_ridge = model.ridge(voxel[:, [-1]], 0)  # 0th index of subj_list\n",
    "            backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "            clipvoxelsTR = clip_voxels0.cpu()\n",
    "\n",
    "    # Perform top-5 retrievals\n",
    "    values_dict = get_top_retrievals(clipvoxelsTR, all_images=images[MST_idx], total_retrievals=5)\n",
    "\n",
    "    # Get the image name for the current index\n",
    "    image_name = vox_image_names[idx]\n",
    "    print(f\"Evaluating image: {image_name}\")\n",
    "\n",
    "    # Process and display the reconstructed image\n",
    "    image_array = np.array(reconsTR)[0]\n",
    "    if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "    # Plot original and reconstructed images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(images[idx].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Reconstructed Image\")\n",
    "    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot original with top-5 retrievals\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(images[idx].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    for j in range(5):\n",
    "        plt.subplot(1, 6, j+2)\n",
    "        plt.title(f\"Retrieval {j+1}\")\n",
    "        plt.imshow(np.array(values_dict[f\"attempt{j+1}\"][0]).transpose(1, 2, 0), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in range(-5, 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.array(all_betas)\n",
    "# z-score the beta map\n",
    "z_mean = np.mean(betas, axis=0)\n",
    "z_std = np.std(betas, axis=0)\n",
    "assert z_std.shape == z_mean.shape, \"z_std and z_mean must have the same shape\"\n",
    "assert z_std.shape == (8627,)\n",
    "betas = (betas - z_mean) / (z_std + 1e-6)  # Normalize betas\n",
    "\n",
    "# Iterate through the first 5 indices of the test set\n",
    "for idx in range(693):\n",
    "    beta_pattern = betas[np.newaxis, np.newaxis, idx, :]  # Add new axes as required\n",
    "    betas_tt = torch.Tensor(beta_pattern).to(\"cpu\")\n",
    "\n",
    "    # Perform reconstructions\n",
    "    reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "\n",
    "    if clipvoxelsTR is None:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            voxel = betas_tt.to(device)\n",
    "            assert voxel.shape[1] == 1\n",
    "            voxel_ridge = model.ridge(voxel[:, [-1]], 0)  # 0th index of subj_list\n",
    "            backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "            clipvoxelsTR = clip_voxels0.cpu()\n",
    "\n",
    "    # Perform top-5 retrievals\n",
    "    values_dict = get_top_retrievals(clipvoxelsTR, all_images=images, total_retrievals=5)\n",
    "\n",
    "    # Get the image name for the current index\n",
    "    image_name = vox_image_names[idx]\n",
    "    print(f\"Evaluating image: {image_name}\")\n",
    "\n",
    "    # Process and display the reconstructed image\n",
    "    image_array = np.array(reconsTR)[0]\n",
    "    if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "    # Plot original and reconstructed images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(images[idx].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Reconstructed Image\")\n",
    "    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot original with top-5 retrievals\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(images[idx].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    for j in range(5):\n",
    "        plt.subplot(1, 6, j+2)\n",
    "        plt.title(f\"Retrieval {j+1}\")\n",
    "        plt.imshow(np.array(values_dict[f\"attempt{j+1}\"][0]).transpose(1, 2, 0), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # save the reconstructed image, retrieved images, clip_voxels, and ground truth image\n",
    "    all_betas_save_path = f\"{derivatives_path}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cae4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_betas).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_norm[0], all_betas_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def vectorized_pearsonr(X, Y):\n",
    "    \"\"\"\n",
    "    Calculates Pearson correlation coefficients and p-values for multiple pairs of arrays.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): A 2D array where each row represents a variable.\n",
    "        Y (numpy.ndarray): A 2D array where each row represents a variable.\n",
    "                           Must have the same number of columns as X.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two NumPy arrays:\n",
    "            - Pearson correlation coefficients (r values).\n",
    "            - p-values.\n",
    "    \"\"\"\n",
    "    if X.shape[1] != Y.shape[1]:\n",
    "        raise ValueError(\"X and Y must have the same number of columns\")\n",
    "\n",
    "    X_mean = np.mean(X, axis=1, keepdims=True)\n",
    "    Y_mean = np.mean(Y, axis=1, keepdims=True)\n",
    "\n",
    "    assert X_mean.shape == (8627,)\n",
    "    assert np.all(X_mean.shape == Y_mean.shape)\n",
    "\n",
    "    X_centered = X - X_mean\n",
    "    Y_centered = Y - Y_mean\n",
    "\n",
    "    numerator = np.sum(X_centered * Y_centered, axis=1)\n",
    "\n",
    "    denominator = np.sqrt(np.sum(X_centered**2, axis=1) * np.sum(Y_centered**2, axis=1))\n",
    "\n",
    "    r = numerator / denominator\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    \n",
    "    #handle potential division by zero\n",
    "    r = np.where(denominator == 0, 0, r)\n",
    "\n",
    "    # Calculate p-values using the t-distribution\n",
    "    t_stat = r * np.sqrt((n - 2) / (1 - r**2))\n",
    "    p = 2 * (1 - stats.t.cdf(np.abs(t_stat), n - 2))\n",
    "    \n",
    "    # Handle cases where r is NaN due to division by zero\n",
    "    p = np.where(np.isnan(p), 1.0, p)\n",
    "\n",
    "    return r, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae220ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = [range(61*i, 63*i) for i in range(1, n_runs+1)]\n",
    "print(rg)\n",
    "indices = [i for r in rg for i in r]\n",
    "print(indices)\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_betas_norm = np.load(betas_path)\n",
    "rt_betas = np.array(all_betas)\n",
    "z_mean = np.mean(rt_betas, axis=0)\n",
    "z_std = np.std(rt_betas, axis=0)\n",
    "rt_betas_norm = (rt_betas - z_mean) / z_std\n",
    "\n",
    "correlations, p = vectorized_pearsonr(offline_betas_norm[indices], rt_betas_norm[indices])\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Mean correlation: {np.mean(correlations):.4f} ± {np.std(correlations):.4f}\")\n",
    "print(f\"Median correlation: {np.median(correlations):.4f}\")\n",
    "print(f\"Min correlation: {np.min(correlations):.4f}\")\n",
    "print(f\"Max correlation: {np.max(correlations):.4f}\")\n",
    "\n",
    "plt.hist(correlations, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Distribution of Correlations Between Real-Time and Offline Betas\")\n",
    "plt.xlabel(\"Correlation Coefficient\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "midx = np.array([v for k,v in image_to_indices.items() if 'MST_pairs' in k])\n",
    "midx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"midx shape:\", midx.shape)\n",
    "print(\"betas shape:\", betas.shape)\n",
    "print(\"betas sample:\", betas[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0d534",
   "metadata": {},
   "source": [
    "#### correlate glmbaseline assumehrf with typed betas (within union mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load assumehrf betas\n",
    "glmbaseline_path = f'{data_path}/testing/glmbaseline/TYPEB_FITHRF.npz'\n",
    "print(glmbaseline_path)\n",
    "glmbaseline = np.load(glmbaseline_path, allow_pickle=True)\n",
    "betas_glmbaseline = np.squeeze(glmbaseline['betasmd']).T\n",
    "print(betas_glmbaseline.shape)\n",
    "\n",
    "# apply union mask\n",
    "brain_mask = nib.load(f'{data_path}/testing/sub-005_ses-03_task-C_brain.nii.gz')\n",
    "tmp = nilearn.masking.unmask(betas_glmbaseline, brain_mask)\n",
    "betas_glmbaseline = nilearn.masking.apply_mask(tmp, mask_img)[:, union_mask]\n",
    "print(betas_glmbaseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load typed betas\n",
    "glm_typed_path = f'{data_path}/testing/TYPED_FITHRF_GLMDENOISE_RR.npz'\n",
    "print(glm_typed_path)\n",
    "glm_typed = np.load(glm_typed_path, allow_pickle=True)\n",
    "betas_typed = np.squeeze(glm_typed['betasmd']).T\n",
    "print(betas_typed.shape)\n",
    "\n",
    "# apply union mask\n",
    "brain_mask = nib.load(f'{data_path}/testing/sub-005_ses-03_task-C_brain.nii.gz')\n",
    "tmp = nilearn.masking.unmask(betas_typed, brain_mask)\n",
    "betas_typed = nilearn.masking.apply_mask(tmp, mask_img)[:, union_mask]\n",
    "print(betas_typed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c83b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score both betas\n",
    "betas_glmbaseline_norm = utils_mindeye.zscore(betas_glmbaseline)\n",
    "betas_typed_norm = utils_mindeye.zscore(betas_typed)\n",
    "\n",
    "correlations, p = vectorized_pearsonr(betas_glmbaseline_norm, betas_typed_norm)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Mean correlation: {np.mean(correlations):.4f} ± {np.std(correlations):.4f}\")\n",
    "print(f\"Median correlation: {np.median(correlations):.4f}\")\n",
    "print(f\"Min correlation: {np.min(correlations):.4f}\")\n",
    "print(f\"Max correlation: {np.max(correlations):.4f}\")\n",
    "\n",
    "plt.hist(correlations, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Correlations Between GLMsingle AssumeHRF and TypeD Betas (z-scored)\")\n",
    "plt.xlabel(\"Correlation Coefficient\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef24bd",
   "metadata": {},
   "source": [
    "#### correlate glmbaseline assumehrf with real-time betas (within union mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43650297",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_glmbaseline_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score both betas\n",
    "betas_glmbaseline_norm = utils_mindeye.zscore(betas_glmbaseline)\n",
    "betas_realtime_norm = utils_mindeye.zscore(np.array(all_betas))\n",
    "\n",
    "correlations, p = vectorized_pearsonr(betas_glmbaseline_norm[-10:], betas_realtime_norm[-10:])\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Mean correlation: {np.mean(correlations):.4f} ± {np.std(correlations):.4f}\")\n",
    "print(f\"Median correlation: {np.median(correlations):.4f}\")\n",
    "print(f\"Min correlation: {np.min(correlations):.4f}\")\n",
    "print(f\"Max correlation: {np.max(correlations):.4f}\")\n",
    "\n",
    "plt.hist(correlations, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Correlations Between GLMsingle AssumeHRF and Real-time Betas (z-scored)\")\n",
    "plt.xlabel(\"Correlation Coefficient\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas[midx][:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_same_diff(same_corrs, diff_corrs, label):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.title(f\"{label} Same/Diff Pearson Correlations\")\n",
    "    plt.plot(np.sort(same_corrs), c='blue', label='same')\n",
    "    if len(diff_corrs) > 0:\n",
    "        plt.plot(np.sort(diff_corrs), c='cyan', label='diff')\n",
    "    else:\n",
    "        print(\"diff_corrs is empty. Skipping diff line.\")\n",
    "    plt.axhline(0, c='k', ls='--')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Pearson R\")\n",
    "    plt.show()\n",
    "\n",
    "def compute_same_diff_correlations(betas, midx, label, plotting=False):\n",
    "    same_corrs = []\n",
    "    diff_corrs = []\n",
    "\n",
    "    for isamp, samp in enumerate(betas[midx][:, 0]):\n",
    "        avg_same_img = []\n",
    "        for i in range(samp.shape[0]):\n",
    "            for j in range(i, samp.shape[0]):\n",
    "                if i != j:\n",
    "                    avg_same_img.append(np.corrcoef(samp[i, :], samp[j, :])[0, 1])\n",
    "        same_corrs.append(avg_same_img[0])\n",
    "\n",
    "        avg_diff_img = []\n",
    "        for isamp_j, samp_j in enumerate(betas[midx][:, 0]):\n",
    "            if isamp_j != isamp:\n",
    "                for i in range(samp_j.shape[0]):\n",
    "                    for j in range(i, samp_j.shape[0]):\n",
    "                        if i != j:\n",
    "                            corr = np.corrcoef(samp[i, :], samp_j[j, :])[0, 1]\n",
    "                            avg_diff_img.append(corr)\n",
    "        if avg_diff_img:\n",
    "            diff_corrs.append(np.mean(avg_diff_img))\n",
    "        else:\n",
    "            print(f\"No valid correlations for isamp {isamp}\")\n",
    "            diff_corrs.append(np.nan)\n",
    "\n",
    "    same_corrs = np.array(same_corrs)\n",
    "    diff_corrs = np.array(diff_corrs)\n",
    "\n",
    "    if plotting:\n",
    "        plot_same_diff(same_corrs, diff_corrs, label=label)\n",
    "    return same_corrs, diff_corrs\n",
    "\n",
    "same_corrs_rt, diff_corrs_rt = compute_same_diff_correlations(betas_norm, midx, label=\"Real-Time Betas\", plotting=True)\n",
    "\n",
    "same_corrs_offline, diff_corrs_offline = compute_same_diff_correlations(all_betas_norm, midx, label=\"Offline Betas\", plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_norm.shape, midx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_same_diff_correlations():\n",
    "    # Generate random betas_norm with shape (693, 8627)\n",
    "    num_samples = 693  # Number of samples\n",
    "    num_voxels = 8627  # Number of voxels\n",
    "    betas_norm = np.random.rand(num_samples, num_voxels)\n",
    "\n",
    "    # Generate random midx with shape (62, 1, 2)\n",
    "    num_images = 62  # Number of unique images\n",
    "    midx = np.random.randint(0, num_samples, size=(num_images, 1, 2))\n",
    "\n",
    "    # Call the function with the generated data\n",
    "    same_corrs, diff_corrs = compute_same_diff_correlations(betas_norm, midx, label=\"Random Test Data\", plotting=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Same correlations:\", same_corrs)\n",
    "    print(\"Diff correlations:\", diff_corrs)\n",
    "\n",
    "# Run the test\n",
    "test_compute_same_diff_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a689fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_same_diff_correlations_balanced(\n",
    "    compute_same_diff_correlations, num_repeats=100):\n",
    "    num_samples = 693\n",
    "    num_voxels = 8627\n",
    "    num_images = 62\n",
    "\n",
    "    all_same_corrs = []\n",
    "    all_diff_corrs = []\n",
    "\n",
    "    for _ in tqdm(range(num_repeats), total=num_repeats):\n",
    "        # Generate random beta values\n",
    "        betas_norm = np.random.rand(num_samples, num_voxels)\n",
    "\n",
    "        # Simulate 62 images with 2 trials each\n",
    "        midx = np.random.randint(0, num_samples, size=(num_images, 1, 2))\n",
    "\n",
    "        # Get all same/diff correlations\n",
    "        same_corrs_all, diff_corrs_all = compute_same_diff_correlations(betas_norm, midx, label='Random Balanced', plotting=False)\n",
    "\n",
    "        all_same_corrs.append(same_corrs_all)\n",
    "        all_diff_corrs.append(diff_corrs_all)\n",
    "\n",
    "    # Convert to arrays and average over trials\n",
    "    all_same_corrs = np.array(all_same_corrs)  # shape (repeats, sample_size)\n",
    "    all_diff_corrs = np.array(all_diff_corrs)\n",
    "\n",
    "    mean_same = all_same_corrs.mean(axis=0)  # mean per-sample across repeats\n",
    "    mean_diff = all_diff_corrs.mean(axis=0)\n",
    "\n",
    "    print(\"Mean same correlation (averaged):\", mean_same.mean())\n",
    "    print(\"Mean diff correlation (averaged):\", mean_diff.mean())\n",
    "\n",
    "    plot_same_diff(same_corrs=mean_same, diff_corrs=mean_diff, label='Random Balanced')\n",
    "\n",
    "    return mean_same, mean_diff\n",
    "\n",
    "test_compute_same_diff_correlations_balanced(compute_same_diff_correlations, num_repeats=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
